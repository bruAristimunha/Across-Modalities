{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#[Re]-A-common-representation-of-time-across-visual-and-auditory-modalities\" data-toc-modified-id=\"[Re]-A-common-representation-of-time-across-visual-and-auditory-modalities-0\"><span class=\"toc-item-num\">0&nbsp;&nbsp;</span>[Re] <a href=\"../docs/a-common-representation.pdf\" target=\"_blank\">A common representation of time across visual and auditory modalities</a></a></span></li><li><span><a href=\"#Abstract-(text)\" data-toc-modified-id=\"Abstract-(text)-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Abstract (text)</a></span></li><li><span><a href=\"#Imports-Packages-(code)\" data-toc-modified-id=\"Imports-Packages-(code)-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Imports Packages (code)</a></span></li><li><span><a href=\"#Introduction:-(text)\" data-toc-modified-id=\"Introduction:-(text)-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Introduction: (text)</a></span></li><li><span><a href=\"#Materials-and-methods-(text-and-code)\" data-toc-modified-id=\"Materials-and-methods-(text-and-code)-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Materials and methods (text and code)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Participants-(text)\" data-toc-modified-id=\"Participants-(text)-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Participants (text)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Set-the-number-of-volunteers-(code)\" data-toc-modified-id=\"Set-the-number-of-volunteers-(code)-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>Set the number of volunteers (code)</a></span></li><li><span><a href=\"#Download-Data-(code)\" data-toc-modified-id=\"Download-Data-(code)-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>Download Data (code)</a></span></li><li><span><a href=\"#Reading-process-(code)\" data-toc-modified-id=\"Reading-process-(code)-4.1.3\"><span class=\"toc-item-num\">4.1.3&nbsp;&nbsp;</span>Reading process (code)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Checking-the-shape-in-arrays\" data-toc-modified-id=\"Checking-the-shape-in-arrays-4.1.3.1\"><span class=\"toc-item-num\">4.1.3.1&nbsp;&nbsp;</span>Checking the shape in arrays</a></span></li></ul></li></ul></li><li><span><a href=\"#Experimental-design-(text)\" data-toc-modified-id=\"Experimental-design-(text)-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Experimental design (text)</a></span></li><li><span><a href=\"#Behavioural-analysis-(text)\" data-toc-modified-id=\"Behavioural-analysis-(text)-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Behavioural analysis (text)</a></span></li><li><span><a href=\"#EEG-recordings-and-pre-processing-(text-and-code)\" data-toc-modified-id=\"EEG-recordings-and-pre-processing-(text-and-code)-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>EEG recordings and pre-processing (text and code)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Frequency-Information-(code)\" data-toc-modified-id=\"Frequency-Information-(code)-4.4.1\"><span class=\"toc-item-num\">4.4.1&nbsp;&nbsp;</span>Frequency Information (code)</a></span></li></ul></li><li><span><a href=\"#Multivariate-pattern-analysis-–-MVPA-(text)\" data-toc-modified-id=\"Multivariate-pattern-analysis-–-MVPA-(text)-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Multivariate pattern analysis – MVPA (text)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Similarity-between-different-trials-and-modalities-(text)\" data-toc-modified-id=\"Similarity-between-different-trials-and-modalities-(text)-4.5.1\"><span class=\"toc-item-num\">4.5.1&nbsp;&nbsp;</span>Similarity between different trials and modalities (text)</a></span></li><li><span><a href=\"#Decoding-the-elapsed-interval-(text)\" data-toc-modified-id=\"Decoding-the-elapsed-interval-(text)-4.5.2\"><span class=\"toc-item-num\">4.5.2&nbsp;&nbsp;</span>Decoding the elapsed interval (text)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Decoding-Code-(code)\" data-toc-modified-id=\"Decoding-Code-(code)-4.5.2.1\"><span class=\"toc-item-num\">4.5.2.1&nbsp;&nbsp;</span>Decoding Code (code)</a></span></li><li><span><a href=\"#Joining-groups-and-X-(code)\" data-toc-modified-id=\"Joining-groups-and-X-(code)-4.5.2.2\"><span class=\"toc-item-num\">4.5.2.2&nbsp;&nbsp;</span>Joining groups and X (code)</a></span></li><li><span><a href=\"#Within-modality-classification-(text)\" data-toc-modified-id=\"Within-modality-classification-(text)-4.5.2.3\"><span class=\"toc-item-num\">4.5.2.3&nbsp;&nbsp;</span>Within modality classification (text)</a></span></li><li><span><a href=\"#Between-modalities-classification-(text)\" data-toc-modified-id=\"Between-modalities-classification-(text)-4.5.2.4\"><span class=\"toc-item-num\">4.5.2.4&nbsp;&nbsp;</span>Between modalities classification (text)</a></span></li></ul></li><li><span><a href=\"#Correlation-of-decoding-and-behaviour-(text)\" data-toc-modified-id=\"Correlation-of-decoding-and-behaviour-(text)-4.5.3\"><span class=\"toc-item-num\">4.5.3&nbsp;&nbsp;</span>Correlation of decoding and behaviour (text)</a></span></li></ul></li></ul></li><li><span><a href=\"#Classification-process\" data-toc-modified-id=\"Classification-process-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Classification process</a></span><ul class=\"toc-item\"><li><span><a href=\"#With-PCA-processing\" data-toc-modified-id=\"With-PCA-processing-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>With PCA processing</a></span></li><li><span><a href=\"#Entre-modalidades.\" data-toc-modified-id=\"Entre-modalidades.-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Entre modalidades.</a></span></li></ul></li><li><span><a href=\"#Checagem-do-rótulos-às-médias\" data-toc-modified-id=\"Checagem-do-rótulos-às-médias-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Checagem do rótulos às médias</a></span></li><li><span><a href=\"#Exportar-para-o-Auto-Enconder()\" data-toc-modified-id=\"Exportar-para-o-Auto-Enconder()-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Exportar para o Auto-Enconder()</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Re] [A common representation of time across visual and auditory modalities](../docs/a-common-representation.pdf)\n",
    "\n",
    "Louise C.Barne<sup>1</sup>, João R. Sato<sup>1</sup>, Raphael Y. de Camargo<sup>1</sup>,Peter M. E. Claessens<sup>1</sup>, Marcelo S. Caetano<sup>1</sup>, André M. Cravo<sup>1*</sup>\n",
    "\n",
    "> <sup>1</sup> Centro de Matemática, Computação e Cognição, Universidade Federal do ABC (UFABC), Rua Arcturus, 03. Bairro Jardim Antares, São Bernardo do Campo, CEP 09606-070 SP, Brazil\n",
    "\n",
    "*andre.cravo@ufabc.edu.br"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract (text)\n",
    "\n",
    "Humans' and non-human animals' ability to process time on the scale of milliseconds and seconds is essential for adaptive behaviour. A central question of how brains keep track of time is how specific temporal information across different sensory modalities is. In the present study, we show that encoding of temporal intervals in auditory and visual modalities are qualitatively similar. Human participants were instructed to reproduce intervals in the range from 750 ms to 1500 ms marked by auditory or visual stimuli. Our behavioural results suggest that, although participants were more accurate in reproducing intervals marked by auditory stimuli, there was a strong correlation in performance between modalities. Using multivariate pattern analysis in scalp EEG, we show that activity during late periods of the intervals was similar within and between modalities. Critically, we show that a multivariate pattern classifier was able to accurately predict the elapsed interval, even when trained on an interval marked by a stimulus of a different sensory modality. Taken together, our results suggest that, while there are differences in the processing of intervals marked by auditory and visual stimuli, they also share a common neural representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keywords: Time perception, Multivariate pattern analysis, EEG, Vision, Audition\n",
    "\n",
    "---\n",
    "\n",
    "Responsible for the reproduction of the results: [Bruno Aristimunha](https://github.com/bruAristimunha).\n",
    "\n",
    "The goals in the work is:\n",
    "    - Find common representations of different sensory modalities;\n",
    "    - Improve the sensitivity of subsequent MVPA analyzes\n",
    "    - Make a reproducible report of the results previously reported;\n",
    "\n",
    "\n",
    "Advisors: [Raphael Y. de Camargo](https://rycamargo.wixsite.com/home) and [André Cravo](https://bv.fapesp.br/pt/pesquisador/65843/andre-mascioli-cravo).\n",
    "\n",
    "***\n",
    "\n",
    "This work follows the structure below: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports Packages (code)\n",
    "Imports packages which are used by jupyter paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:25:46.949322Z",
     "start_time": "2020-09-01T13:25:46.443611Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brain/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/ipykernel_launcher.py:3: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, LeaveOneOut\n",
    "from tqdm.autonotebook import tqdm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report, cohen_kappa_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import scipy.io as sio\n",
    "import scipy as sc\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# %matplotlib outline\n",
    "sns.set()\n",
    "\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "sys.path.append(\"../src/data/\")\n",
    "sys.path.append(\"../src/tools/\")\n",
    "sys.path.append(\"../src/models/\")\n",
    "sys.path.append(\"../src/external/\")\n",
    "sys.path.append(\"../src/visualization/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:25:47.233834Z",
     "start_time": "2020-09-01T13:25:46.950342Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 items had no tests:\n",
      "    __main__\n",
      "0 tests in 1 items.\n",
      "0 passed and 0 failed.\n",
      "Test passed.\n"
     ]
    }
   ],
   "source": [
    "from file import *\n",
    "from conversion import *\n",
    "from exposure import _exposure_1, _exposure_2, _exposure_1_bad, _exposure_2_bad, get_group_time, fixing_bad_trials, get_last_125ms\n",
    "from classification import *\n",
    "from statistical_tests import *\n",
    "from report import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: (text)\n",
    "\n",
    "The ability to estimate time is essential for humans and non-human animals to interact with their environment ([Buhusi and Meck, 2005](https://www.sciencedirect.com/science/article/pii/S0028393218304913%23bib5&sa=D&ust=1580959905399000),[ ](https://www.sciencedirect.com/science/article/pii/S0028393218304913%23bib27&sa=D&ust=1580959905399000)[Mauk and Buonomano, 2004](https://www.sciencedirect.com/science/article/pii/S0028393218304913%23bib27&sa=D&ust=1580959905400000),[ ](https://www.sciencedirect.com/science/article/pii/S0028393218304913%23bib28&sa=D&ust=1580959905400000)[Merchant et al., 2013a](https://www.sciencedirect.com/science/article/pii/S0028393218304913%23bib28&sa=D&ust=1580959905401000)). Intervals in the range of hundreds of milliseconds to seconds are critical for sensory and motor processing, learning, and cognition ([Buhusi and Meck, 2005](https://www.sciencedirect.com/science/article/pii/S0028393218304913%23bib5&sa=D&ust=1580959905401000),[ ](https://www.sciencedirect.com/science/article/pii/S0028393218304913%23bib27&sa=D&ust=1580959905402000)[Mauk and Buonomano, 2004](https://www.sciencedirect.com/science/article/pii/S0028393218304913%23bib27&sa=D&ust=1580959905402000),[ ](https://www.sciencedirect.com/science/article/pii/S0028393218304913%23bib28&sa=D&ust=1580959905403000)[Merchant et al., 2013a](https://www.sciencedirect.com/science/article/pii/S0028393218304913%23bib28&sa=D&ust=1580959905403000)). However, the mechanisms underlying temporal processing in this range are still largely discussed. A central unanswered question is whether temporal processing depends on dedicated or intrinsic circuits ([Ivry and Schlerf, 2008](https://www.sciencedirect.com/science/article/pii/S0028393218304913%23bib16&sa=D&ust=1580959905404000)). Dedicated models propose that temporal perception depends on central specialised mechanisms, as an internal clock, that create a unified perception of time ([Ivry and Schlerf, 2008](https://www.sciencedirect.com/science/article/pii/S0028393218304913%23bib16&sa=D&ust=1580959905404000)). This class of models can account for behavioural findings such as correlations in performance for some temporal tasks ([Keele et al., 1985](https://www.sciencedirect.com/science/article/pii/S0028393218304913%23bib20&sa=D&ust=1580959905405000)) and the observation that learning to discriminate a temporal interval in one sensory modality can sometimes be transferred to other modalities ([Bueti and Buonomano, 2014](https://www.sciencedirect.com/science/article/pii/S0028393218304913%23bib4&sa=D&ust=1580959905405000)).\n",
    "\n",
    "Intrinsic models of time propose that a variety of neural circuits distributed across the brain are capable of temporal processing. One of the most known examples is the state-dependent network - SDN ([Mauk and Buonomano, 2004](https://www.sciencedirect.com/science/article/pii/S0028393218304913%23bib27&sa=D&ust=1580959905406000)). Within this framework, neural circuits can take advantage of the natural temporal evolution of its states to keep track of time ([Mauk and Buonomano, 2004](https://www.sciencedirect.com/science/article/pii/S0028393218304913%23bib27&sa=D&ust=1580959905407000)). One of the main advantages of such models is that they can explain the known differences of temporal processing across sensory modalities ([van Wassenhove, 2009](https://www.sciencedirect.com/science/article/pii/S0028393218304913%23bib38&sa=D&ust=1580959905407000)) and that learning a specific interval does not commonly improve temporal performance in other intervals ([Bueti and Buonomano, 2014](https://www.sciencedirect.com/science/article/pii/S0028393218304913%23bib4&sa=D&ust=1580959905408000)).\n",
    "\n",
    "Given that both dedicated and intrinsic views can account for some results while not explaining others, there has been an increase in interest in hybrid models, according to which local task-dependent areas interact with a higher central timing system ([Merchant et al., 2013a](https://www.sciencedirect.com/science/article/pii/S0028393218304913%23bib28&sa=D&ust=1580959905409000),[ ](https://www.sciencedirect.com/science/article/pii/S0028393218304913%23bib39&sa=D&ust=1580959905409000)[Wiener et al., 2011](https://www.sciencedirect.com/science/article/pii/S0028393218304913%23bib39&sa=D&ust=1580959905410000)). The main advantage of hybrid models is that they can explain why performance in some timing tasks seems to be correlated across participants, while still exhibiting modality and task-related differences ([Merchant et al., 2008b](https://www.sciencedirect.com/science/article/pii/S0028393218304913%23bib33&sa=D&ust=1580959905410000),[ ](https://www.sciencedirect.com/science/article/pii/S0028393218304913%23bib32&sa=D&ust=1580959905411000)[Merchant et al., 2008a](https://www.sciencedirect.com/science/article/pii/S0028393218304913%23bib32&sa=D&ust=1580959905411000)).\n",
    "\n",
    "In humans, studies that investigate these different models employ a variety of methods, such as behavioural, neuroimaging and neuropharmacological manipulations, on healthy participants and neurological patients ([Coull et al., 2011](https://www.sciencedirect.com/science/article/pii/S0028393218304913%23bib9&sa=D&ust=1580959905412000),[ ](https://www.sciencedirect.com/science/article/pii/S0028393218304913%23bib16&sa=D&ust=1580959905413000)[Ivry and Schlerf, 2008](https://www.sciencedirect.com/science/article/pii/S0028393218304913%23bib16&sa=D&ust=1580959905413000),[ ](https://www.sciencedirect.com/science/article/pii/S0028393218304913%23bib21&sa=D&ust=1580959905414000)[Kononowicz et al., 2016](https://www.sciencedirect.com/science/article/pii/S0028393218304913%23bib21&sa=D&ust=1580959905414000),[ ](https://www.sciencedirect.com/science/article/pii/S0028393218304913%23bib28&sa=D&ust=1580959905415000)[Merchant et al., 2013a](https://www.sciencedirect.com/science/article/pii/S0028393218304913%23bib28&sa=D&ust=1580959905415000),[ ](https://www.sciencedirect.com/science/article/pii/S0028393218304913%23bib39&sa=D&ust=1580959905416000)[Wiener et al., 2011](https://www.sciencedirect.com/science/article/pii/S0028393218304913%23bib39&sa=D&ust=1580959905416000)). Although the high temporal resolution of EEG should in principle be optimal to track neural processing during temporal tasks, the contribution of these methods has been controversial. One of the main difficulties is the absence of a clear electrophysiological correlate of temporal processing (for a recent review see ([Kononowicz et al., 2016](https://www.sciencedirect.com/science/article/pii/S0028393218304913%23bib21&sa=D&ust=1580959905417000))). This lack of electrophysiological markers makes it hard to judge, for example, whether temporal processing in different modalities share a common representation ([N'Diaye et al., 2004](https://www.sciencedirect.com/science/article/pii/S0028393218304913%23bib34&sa=D&ust=1580959905417000)).\n",
    "\n",
    "In a recent study, we have shown that multivariate pattern analysis (MVPA) can reveal spatiotemporal dynamics of brain activity related to temporal processing ([Bueno et al., 2017](https://www.sciencedirect.com/science/article/pii/S0028393218304913%23bib3&sa=D&ust=1580959905418000)). Multivariate approaches can take advantage of small differences in the signal across electrodes that might not be detectable using classical EEG methods. These pattern recognition methods allow the assessment of whether brain states evoked by different tasks, stimuli and sensory modalities are qualitatively similar.\n",
    "\n",
    "In the present study, we investigated whether encoding of temporal intervals in different sensory modalities is qualitatively similar. Our behavioural results suggest that, although participants are more accurate in reproducing intervals marked by auditory stimuli, there is a strong correlation over observers in performance between modalities. Critically, we show that a multivariate pattern classifier based on EEG activity can predict the elapsed interval, even when trained on an interval marked by a different sensory modality. Taken together, our results suggest that, while there are differences in the processing of intervals marked by auditory and visual stimuli, they also share a common neural representation.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Materials and methods (text and code)\n",
    "\n",
    "## Participants (text)\n",
    "\n",
    "Twenty volunteers (age range, 18–30 years; 11 female) gave informed consent to participate in this study. All of them had normal or corrected-to-normal vision and were free from psychological or neurological diseases. The experimental protocol was approved by The Research Ethics Committee of the Federal University of ABC. All experiments were performed in accordance with the approved guidelines and regulations.\n",
    "\n",
    "### Set the number of volunteers (code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:25:47.236513Z",
     "start_time": "2020-09-01T13:25:47.234752Z"
    }
   },
   "outputs": [],
   "source": [
    "N_PEOPLE = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data (code)\n",
    "\n",
    "The EEG data obtained from the experiment reported above and the meta-information are required for reproduction. These files have 2.1G GB and you do need them for all the analysis in the Jupyter Paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:25:47.307005Z",
     "start_time": "2020-09-01T13:25:47.237302Z"
    }
   },
   "outputs": [],
   "source": [
    "PATH_AUD = '../data/raw/aud'\n",
    "PATH_VIS = '../data/raw/vis'\n",
    "PATH_INFO = '../data/raw/info_'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading process (code)\n",
    "\n",
    "TO-DO: short text describing the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:26:03.311518Z",
     "start_time": "2020-09-01T13:25:47.308597Z"
    }
   },
   "outputs": [],
   "source": [
    "data_aud, data_vis, CHANNEL_NAMES = read_file(PATH_AUD, PATH_VIS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the shape in arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:26:03.315300Z",
     "start_time": "2020-09-01T13:26:03.313082Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Aud with shape: (20, 539, 64, 240) (individual, raw, channels, trial)\n",
      "Data Vis with shape: (20, 539, 64, 240) (individual, raw, channels, trial)\n"
     ]
    }
   ],
   "source": [
    "print(\"Data Aud with shape: {} (individual, raw, channels, trial)\".format(data_aud.shape))\n",
    "print(\"Data Vis with shape: {} (individual, raw, channels, trial)\".format(data_vis.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental design (text)\n",
    "\n",
    "The experiment consisted of a temporal reproduction task. The stimuli were presented using the Psychtoolbox v.3.0 package ([Brainard, 1997](https://www.sciencedirect.com/science/article/pii/S0028393218304913#bib2)) on a 20-in. CRT monitor with a vertical refresh rate of 60 Hz, placed 50 cm in front of the participant.\n",
    "\n",
    "Each trial started with a fixation point that participants fixated throughout the trial. After a delay (1.5 s), two flashes or tones were presented, separated by a sample interval (measured between tone or flash onsets). After a random delay (1.5–2.5 s), volunteers were re-exposed to the same interval. After another random delay (1.5–2.5 s), a ready stimulus was presented to the participant, indicating the beginning of the reproduction task [Figure 1](Figure 1). Volunteers had to reproduce the exposed interval, initiated by the ready stimulus (RS) and ended by a stimulus caused by a button press (R1).\n",
    "\n",
    "![Figure 1](../reports/figures/Figure_01_temporal_reproduction_task.jpg)\n",
    "**Figure 1 | Temporal reproduction task.** (**A**) Sequence of events during a trial. Each trial consisted of two equal empty intervals (between 750 ms and 1500 ms), marked by two stimuli. In auditory blocks, the interval was marked by two brief tones (1000 Hz, 100 ms), while in visual blocks the interval was marked by two flashes (0.5° of visual angle, 100 ms). Participants were instructed to reproduce the interval at the end of each trial. .\n",
    "\n",
    "\n",
    "In auditory blocks, the tones consisted of 1000 Hz tones (100 ms duration), while the ready and end stimuli consisted of 500 Hz tones. In visual blocks, the flashes that marked the interval consisted of yellow 0.5° of visual angle discs (100 ms duration), while the ready and end stimuli consisted of magenta flashes of the same size. No direct feedback was given.\n",
    "\n",
    "The sample intervals ranged between 750 ms and 1500 ms and were uniformly distributed. Each block (visual or auditory) consisted of 120 trials. Half of the participants performed the visual block first, while the other half performed the auditory block first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behavioural analysis (text)\n",
    "\n",
    "Events in which intervals were reproduced as longer than double the sample interval or shorter than half the sample interval were considered errors and excluded from further analyses. The proportion of errors was low for both modalities (auditory: $0.0042 \\pm 0.0011$; maximum proportion of rejected trials per participant=$0.0167$; visual: $0.0092 \\pm 0.0031$; maximum proportion of rejected trials per participant $= 0.0583$)\n",
    "\n",
    "Similarly to previous studies ([Cicchini et al., 2012](), [Jazayeri and Shadlen, 2010]() ), the total error in the reproduction task was partitioned into two components: the average bias (BIAS) and the average variance ($\\text{VAR}$). These two metrics are directly related to the overall mean squared error (MSE). To calculate these components, sample intervals were first binned into six equally sized bins and, for each bin, an estimate of both measures were calculated. The BIAS for each bin was calculated as the average difference between the reproduced interval and the sample intervals. The $\\text{VAR}$ for each bin was calculated as the variance of the difference between reproduced and real intervals. The final estimate of the BIAS was calculated as the root mean square of the BIAS across bins and of the $\\text{VAR}$ as the average $\\text{VAR}$ across bins ([Jazayeri and Shadlen, 2010]()). For ease of interpretation and comparison with previous studies, the $\\text{VAR}$ values were plotted using its square root $\\sqrt{\\text{VAR}}$.\n",
    "\n",
    "\n",
    "We further calculated a regression index ($\\text{RI}$) to index the tendency of reproduced intervals to regress towards the mean sample. This index was calculated as the difference in slope between the best linear fit on the reproduced interval and perfect performance ([Cicchini et al., 2012]()). This measure varies from $0$ (perfect performance) to $1$ (complete regression to the mean, after allowing for a constant bias). The same linear fit between real and reproduced intervals was used to calculate the indifference points for both modalities. This point refers to the physical interval where durations are reproduced veridically.\n",
    "\n",
    "To analyse the scalar property of time, we computed the slope of the generalised Weber function ([García-Garibay et al., 2016](), [Getty, 1975](), [Ivry and Hazeltine, 1995]()). A linear regression between the variance of the reproduced intervals and the mean subjective duration squared was performed as follows: (1)\n",
    "\n",
    "\n",
    "where $k$ is the slope that approximates the Weber fraction and $\\sigma^2_{\\text{indep}}$ is a constant representing the time-independent component of the variability. To account for the systematic bias found in the reproduced intervals, we used an approach similar to [García-Garibay et al. (2016)] in which\n",
    "\n",
    "was computed based on the linear fit between real and reproduced intervals.\n",
    "\n",
    "All measures were calculated separately for each participant and condition (auditory and visual). At the group level, comparisons between the calculated parameters were done using paired t-tests (two-sided). To investigate correlations across participants for all three measures, Pearson correlations were calculated between the values of each measure in visual and auditory conditions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EEG recordings and pre-processing (text and code)\n",
    "\n",
    "EEG was recorded continuously from 64 ActiCap Electrodes (Brain Products) at 1000 Hz by a QuickAmp amplifier (Brain Products). All sites were referenced to FCz and grounded to AFz. The electrodes were positioned according to the International 10–10 system. Additional bipolar electrodes registered the electrooculogram (EOG).\n",
    "\n",
    "EEG pre-processing was carried out using BrainVision Analyzer (Brain Products). All data were down-sampled to 250 Hz and re-referenced to the average activity across electrodes. For eye movement artefact rejection, an independent component analysis (ICA) was performed on filtered (Butterworth Zero Phase Filter between 0.05.05 Hz and 30 Hz) and segmented (−200 to 2000 ms relative to S1) data. For the ICA, epochs were baselined based on the average activity of the entire trial. Eye related components were identified by comparing individual ICA components with EOG channels and by visual inspection. The average proportion of rejected trials for each participant corresponded to 0.0613. For all further analyses, epochs were baseline corrected based on the period between −200 ms and 0 ms relative to S1 presentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Information (code)\n",
    "\n",
    "The data set was originally recorded in 1000 Hz and down-sampled to 250 Hz;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:26:03.406685Z",
     "start_time": "2020-09-01T13:26:03.316189Z"
    }
   },
   "outputs": [],
   "source": [
    "N_CHANNELS = 64\n",
    "#N_CHANNELS = 62\n",
    "\n",
    "\n",
    "ORIGINAL_FREQUENCY = 1000\n",
    "\n",
    "DOWN_SAMPLED_FREQUENCY = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate pattern analysis – MVPA (text)\n",
    "\n",
    "### Similarity between different trials and modalities (text)\n",
    "\n",
    "\n",
    "Not reproduced **yet**.\n",
    "\n",
    "<!---\n",
    "To calculate the similarity across trials we used a bootstrap approach. For each participant and comparison of interest, data from all intervals that lasted at least 1.125 s were divided into two groups of trials and averaged across trials per group ([Bueno et al., 2017]()). Then, for each time point, data from each split (two-row vectors with the averaged amplitude values of all 62 electrodes) were compared using a Pearson correlation. This procedure was repeated 5000 times for each participant and Fisher transformed coefficients were averaged across permutations for each participant and comparison of interest.\n",
    "\n",
    "At the group level, EEG-analyses were implemented non-parametrically ([Maris and Oostenveld, 2007](), [Wolff et al., 2015]()) with sign-permutation tests. For each time-point, the Fisher transformed coefficients for a random half of the participants were multiplied by $-1$. The resulting distribution was used to calculate the p-value of the null-hypothesis that the mean value was equal to $0$. Cluster-based permutation tests were then used to correct for multiple comparisons across time using $5000$ permutations, with a cluster-forming threshold of p$<0.05$ ([Maris and Oostenveld, 2007]()). The sum of the values within a cluster was used as the cluster-level statistic. The significance threshold was set at p$<0.05$; all tests were one-sided.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding the elapsed interval (text)\n",
    "\n",
    "To investigate whether information about the elapsed interval could be decoded from activity across electrodes, we used a Naive Bayes classifier to perform a multiclass classification ([Grootswagers et al., 2017]()). For all classifications, activity from the last 125 ms of the elapsed interval (−125 ms to 0 relative to S2) was averaged for each electrode. Data for each exposure (E1 and E2) were analysed separately. Intervals were divided into six equally sized bins (125 ms each).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoding Code (code)\n",
    "\n",
    "\n",
    "At this point we understand that:\n",
    "\n",
    "> We should divide the exposures. Consulting the authors by definition in trial odd are the first exposure, and trial even the second exposure. For this we develop the functions `_exposure_1`, `_exposure_2`.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:26:03.482100Z",
     "start_time": "2020-09-01T13:26:03.411369Z"
    }
   },
   "outputs": [],
   "source": [
    "## Splitting the exposures for all the individuals\n",
    "\n",
    "aud_1 = _exposure_1(data_aud)\n",
    "\n",
    "aud_2 = _exposure_2(data_aud) \n",
    "\n",
    "vis_1 = _exposure_1(data_vis)\n",
    "\n",
    "vis_2 = _exposure_2(data_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:26:15.047095Z",
     "start_time": "2020-09-01T13:26:03.486276Z"
    }
   },
   "outputs": [],
   "source": [
    "bad_trials_aud, bad_trials_vis = get_bad_trials(PATH_AUD, PATH_VIS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:26:15.057444Z",
     "start_time": "2020-09-01T13:26:15.048119Z"
    }
   },
   "outputs": [],
   "source": [
    "## Splitting the exposures for all the individuals\n",
    "\n",
    "bad_aud_1 = _exposure_1_bad(bad_trials_aud, 'Aud')\n",
    "\n",
    "bad_aud_2 = _exposure_2_bad(bad_trials_aud, 'Aud') \n",
    "\n",
    "bad_vis_1 = _exposure_1_bad(bad_trials_vis, 'Vis')\n",
    "\n",
    "bad_vis_2 = _exposure_2_bad(bad_trials_vis, 'Vis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:26:15.238116Z",
     "start_time": "2020-09-01T13:26:15.058373Z"
    }
   },
   "outputs": [],
   "source": [
    "bad_vis_comport = get_bad_trials_comportamental('vis')\n",
    "bad_aud_comport = get_bad_trials_comportamental('aud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:26:15.254586Z",
     "start_time": "2020-09-01T13:26:15.239068Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_aud_1, clean_aud_2, clean_vis_1, clean_vis_2 = fixing_bad_trials(\n",
    "    bad_aud_1, bad_aud_2, bad_vis_1, bad_vis_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:26:15.311102Z",
     "start_time": "2020-09-01T13:26:15.255474Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_aud_1 = clean_aud_1.append(bad_aud_comport,ignore_index=True)\n",
    "clean_aud_2 = clean_aud_2.append(bad_aud_comport,ignore_index=True)\n",
    "\n",
    "clean_vis_1 = clean_vis_1.append(bad_vis_comport,ignore_index=True)\n",
    "clean_vis_2 = clean_vis_2.append(bad_vis_comport,ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For each trial, we will get only the last $125$ ms before $S2$;\n",
    "\n",
    "> The rest is discarded;\n",
    "\n",
    "> We must evaluate the mean value in the last $125$ ms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the sampled frequency of $250$ Hz, there will be one point every $4$ milliseconds. If we want $ 125 $ milliseconds just divide it and take the ratio $\\frac{125 \\text{ms}}{4 \\text{ms}}$ to get the number of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:26:15.378553Z",
     "start_time": "2020-09-01T13:26:15.312245Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the sampling frequency we will then have a total of 31.25 points\n"
     ]
    }
   ],
   "source": [
    "print(\"Given the sampling frequency we will then have a total of {} points\".format(125/4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For numerical simplicity we adopted the number of points as $32$. With the number of points established we implement the functions: `read_time_delay`, `get_last_125ms`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:26:15.499494Z",
     "start_time": "2020-09-01T13:26:15.383303Z"
    }
   },
   "outputs": [],
   "source": [
    "indice_s2_vis = get_time_delay('vis')\n",
    "indice_s2_aud = get_time_delay('aud')\n",
    "\n",
    "time_s2_vis = get_time_delay('vis', export_as_indice=False)\n",
    "time_s2_aud = get_time_delay('aud', export_as_indice=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:26:15.707792Z",
     "start_time": "2020-09-01T13:26:15.501009Z"
    }
   },
   "outputs": [],
   "source": [
    "aud_1_average = list(map(get_last_125ms, aud_1, indice_s2_aud))\n",
    "aud_2_average = list(map(get_last_125ms, aud_2, indice_s2_aud))\n",
    "\n",
    "\n",
    "vis_1_average = list(map(get_last_125ms, vis_1, indice_s2_vis))\n",
    "vis_2_average = list(map(get_last_125ms, vis_2, indice_s2_vis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For each trial we must put in one of the six group depending on the trial length:\n",
    "\n",
    "| Group ID \t| Begin(ms) \t| End(ms) \t|\n",
    "|:--------:\t|:---------:\t|:-------:\t|\n",
    "|     1    \t|    750    \t|   875   \t|\n",
    "|     2    \t|    875    \t|   1000  \t|\n",
    "|     3    \t|    1000   \t|   1125  \t|\n",
    "|     4    \t|    1125   \t|   1250  \t|\n",
    "|     5    \t|    1250   \t|   1375  \t|\n",
    "|     6    \t|    1375   \t|   1500  \t|\n",
    "\n",
    "The ranges are defined as $[,[ $. We reused the previously developed function ``read_time_delay``, and just deploy ``categorize``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:26:15.804161Z",
     "start_time": "2020-09-01T13:26:15.708742Z"
    }
   },
   "outputs": [],
   "source": [
    "classes_aud, classes_vis = get_group_time(time_s2_aud, time_s2_vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joining groups and X (code)\n",
    "\n",
    "For ease when joining the class we developed the function `to_DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:26:16.122706Z",
     "start_time": "2020-09-01T13:26:15.805086Z"
    }
   },
   "outputs": [],
   "source": [
    "#Getting in format of dataframe:\n",
    "\n",
    "df_aud_1_aver = to_DataFrame(aud_1_average,classes_aud,CHANNEL_NAMES)\n",
    "df_aud_2_aver = to_DataFrame(aud_2_average,classes_aud,CHANNEL_NAMES)\n",
    "\n",
    "df_vis_1_aver = to_DataFrame(vis_1_average,classes_vis,CHANNEL_NAMES)\n",
    "df_vis_2_aver = to_DataFrame(vis_2_average,classes_vis,CHANNEL_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Export to Alberto**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:26:16.234898Z",
     "start_time": "2020-09-01T13:26:16.123727Z"
    }
   },
   "outputs": [],
   "source": [
    "df_aud_1_aver = merge_and_clean(df_aud_1_aver,clean_aud_1)\n",
    "df_aud_2_aver = merge_and_clean(df_aud_2_aver,clean_aud_2)\n",
    "\n",
    "\n",
    "df_vis_1_aver = merge_and_clean(df_vis_1_aver,clean_vis_1)\n",
    "df_vis_2_aver = merge_and_clean(df_vis_2_aver,clean_vis_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:26:16.761867Z",
     "start_time": "2020-09-01T13:26:16.235753Z"
    }
   },
   "outputs": [],
   "source": [
    "df_aud_1_aver.to_csv(\"../data/processed/auditory_exposure_1.csv\", index=None)\n",
    "df_aud_2_aver.to_csv(\"../data/processed/auditory_exposure_2.csv\", index=None)\n",
    "\n",
    "\n",
    "df_vis_1_aver.to_csv(\"../data/processed/visual_exposure_1.csv\", index=None)\n",
    "df_vis_2_aver.to_csv(\"../data/processed/visual_exposure_2.csv\", index=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Within modality classification (text)\n",
    "\n",
    "The pre-processing and decoding procedures followed standard guidelines ([Grootswagers et al., 2017](), [Lemm et al., 2011]()). A leave-one-out method was used, although similar results were obtained when using a 10-fold cross-validation. For each test trial, data from all other trials were used as the training set. A principal component analysis (PCA) was used to reduce the dimensionality of the data ([Grootswagers et al., 2017]()). The PCA transformation was computed on the training data and the components that accounted for 99% of the variance were retained. The estimated coefficients from the training data were applied to the test data. On average, the number of components used was of 37.17 $\\pm$\n",
    "1.43 (mean s.e.m., maximum number of components=47, minimum number of components=17).\n",
    "\n",
    "Decoding performance was summarised with Cohen's quadratic weighted kappa coefficient ([Cohen, 1968]()). The weighted kappa gives more weight to disagreements for categories that are further apart. It assumes values from −1 to 1, with 1 indicating perfect agreement between predicted and decoded label and 0 indicating chance ([Cohen, 1968]()).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:26:20.665164Z",
     "start_time": "2020-09-01T13:26:16.762812Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d594609fb43f4797891ed7cfb166d31a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2272.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-d5df72f6b12c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresu_aud_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassification_within_modality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_aud_1_aver\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Auditory'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'E1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mresu_aud_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassification_within_modality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_aud_2_aver\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Auditory'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'E2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mresu_vis_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassification_within_modality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_vis_1_aver\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Visual'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'E1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mresu_vis_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassification_within_modality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_vis_2_aver\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Visual'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'E2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Project_n/Across-Modalities/src/models/classification.py\u001b[0m in \u001b[0;36mclassification_within_modality\u001b[0;34m(dataFrame, categoria, exposure)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mX_train_pca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_without_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mX_test_pca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_without_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/sklearn/decomposition/_base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;31m# DataFrame), and store them. If not, store None.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mdtypes_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtypes\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__array__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m         \u001b[0mdtypes_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdtypes_orig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdtypes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5635\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5637\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5639\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    312\u001b[0m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_cast_failure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSingleBlockManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "resu_aud_1 = classification_within_modality(df_aud_1_aver,'Auditory','E1')\n",
    "resu_aud_2 = classification_within_modality(df_aud_2_aver,'Auditory','E2')\n",
    "\n",
    "resu_vis_1 = classification_within_modality(df_vis_1_aver,'Visual','E1')\n",
    "resu_vis_2 = classification_within_modality(df_vis_2_aver,'Visual','E2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Between modalities classification (text)\n",
    "\n",
    "The between modality decoding (training the classifier on data from one modality and testing on the other) followed a similar structure as the within-modality decoding. Given that participants performed the first half of the experiment with one modality and the second with the other, the effects of non-stationarity are low ([Lemm et al., 2011]()). Thus, data from all trials of one modality were used as the training set and tested against all trials of the other modality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation of decoding and behaviour (text)\n",
    "\n",
    "Similarly to our previous decoding analysis, activity from the last 125 ms of the elapsed interval (−125 ms to 0 relative to S2) was used. For each test trial, a Naive Bayes classifier was trained on trials that were shorter (at least 25 ms and up to 125 ms shorter) and longer (at least 25 ms and up to 125 ms longer) than that the physical duration of that given test trial. This classification was applied to all trials between 875 ms and 1375 ms, so that all trials had a similar number of training trials.\n",
    "\n",
    "4\n",
    "\n",
    "Based on this classification, trials were further classified into three categories: (S/S) when in both presentations (E1 and E2) the trial was classified as shorter; (S/L or L/S) when in only one of the two presentations the trial was classified as longer and (L/L) when in both presentation the trials were classified as longer.\n",
    "\n",
    "To test whether classification might explain participants' performance, we first fitted, for each participant, a linear function relating the physical interval and participants' reproduced interval. The residuals from this linear fit were stored and separated as a function of the three decoding categories mentioned above. Notice that the linear fit on the behavioural results naturally takes into account participants tendency to regress towards the mean. Thus, this analysis allows isolating if trials that were encoded as shorter or longer were reproduced as shorter or longer than participants' average reproduction for that interval duration.\n",
    "Statistical analysis\n",
    "\n",
    "All t-tests, correlations, ANOVAs and effect sizes (Cohen's d for t-tests and omega-squared for ANOVAs) were calculated using JASP (JASP Team, 2017). The p-values were adjusted, whenever appropriate, for violations of the sphericity assumption using the Greenhouse-Geisser correction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification process\n",
    "\n",
    "## With PCA processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each circle shows, for each participant and tested bin, the most common output of the classifier (mode). The continuous line shows the median across participants and error bars show the lower quartile (25th percentile) and upper quartile (75th percentile) across participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:26:20.666357Z",
     "start_time": "2020-09-01T13:25:46.566Z"
    }
   },
   "outputs": [],
   "source": [
    "merge_within, merge_within_mode = merge_export(resu_aud_1,\n",
    "                                      resu_aud_2,\n",
    "                                      resu_vis_1,\n",
    "                                      resu_vis_2,\n",
    "                                      \"../data/processed/resu_fig5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:26:20.668286Z",
     "start_time": "2020-09-01T13:25:46.571Z"
    }
   },
   "outputs": [],
   "source": [
    "ck_within = cohen_kappa(merge_within, plot_option=False, title='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 5. C) Agreement between decoded and real interval indexed by Cohen's kappa. The left panel shows classification performance when test trials were of the same modality as the training trials while right panels show performance when the test trials were of a different modality than the training trials. Violin plots show the probability density across participants; line markers represent the median. Circles inside the violin plots show data from each participant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:26:20.670135Z",
     "start_time": "2020-09-01T13:25:46.577Z"
    }
   },
   "outputs": [],
   "source": [
    "print_classification_by_mod(merge_within)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---------\n",
    " \n",
    " \n",
    "## Entre modalidades.\n",
    " \n",
    " \n",
    " \n",
    " ---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:26:20.671996Z",
     "start_time": "2020-09-01T13:25:46.581Z"
    }
   },
   "outputs": [],
   "source": [
    "prever_vis_1 = classification_across_modality(\n",
    "    df_aud_1_aver, df_vis_1_aver, inp='Visual', exp='E1')\n",
    "prever_aud_1 = classification_across_modality(\n",
    "    df_vis_1_aver, df_aud_1_aver, inp='Auditory', exp='E1')\n",
    "\n",
    "prever_vis_2 = classification_across_modality(\n",
    "    df_aud_2_aver, df_vis_2_aver, inp='Visual', exp='E2')\n",
    "prever_aud_2 = classification_across_modality(\n",
    "    df_vis_2_aver, df_aud_2_aver, inp='Auditory', exp='E2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:26:20.673846Z",
     "start_time": "2020-09-01T13:25:46.586Z"
    }
   },
   "outputs": [],
   "source": [
    "merge_across, merge_across_mode = merge_export(prever_aud_1, prever_aud_2,\n",
    "                                               prever_vis_1, prever_vis_2,\n",
    "                                               '../data/processed/resu_fig5b.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:26:20.675698Z",
     "start_time": "2020-09-01T13:25:46.590Z"
    }
   },
   "outputs": [],
   "source": [
    "ck_across = cohen_kappa(merge_across, plot_option=False, title='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:26:20.677546Z",
     "start_time": "2020-09-01T13:25:46.595Z"
    }
   },
   "outputs": [],
   "source": [
    "print_classification_by_mod(merge_across)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:26:20.679403Z",
     "start_time": "2020-09-01T13:25:46.600Z"
    }
   },
   "outputs": [],
   "source": [
    "make_figure(merge_across_mode, merge_within_mode, ck_across, ck_within)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:26:20.681255Z",
     "start_time": "2020-09-01T13:25:46.605Z"
    }
   },
   "outputs": [],
   "source": [
    "t_test(ck_within)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:26:20.683105Z",
     "start_time": "2020-09-01T13:25:46.609Z"
    }
   },
   "outputs": [],
   "source": [
    "t_test(ck_across)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checagem do rótulos às médias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:26:20.684964Z",
     "start_time": "2020-09-01T13:25:46.615Z"
    }
   },
   "outputs": [],
   "source": [
    "contagem_pessoas_classe = df_aud_1_aver.groupby(\n",
    "    ['people', 'group']).size().unstack()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "\n",
    "ax = contagem_pessoas_classe.T.plot.bar(ax=ax)\n",
    "\n",
    "fig = fig.suptitle(\"Distribuição das classes nos trials por pessoas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:26:20.686815Z",
     "start_time": "2020-09-01T13:25:46.620Z"
    }
   },
   "outputs": [],
   "source": [
    "contagem_classe = df_aud_1_aver.groupby(['group']).size()\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "\n",
    "ax = contagem_classe.T.plot.bar(ax=ax)\n",
    "\n",
    "fig = fig.suptitle(\"Distribuição das classes nos trials de forma geral\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exportar para o Auto-Enconder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:26:20.688685Z",
     "start_time": "2020-09-01T13:25:46.625Z"
    }
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from numpy import array, divide, average, concatenate\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import doctest\n",
    "\n",
    "N_TRIALS = 120\n",
    "\n",
    "def get_last_125ms(exposure, indice_exposure, use_average=True, last_32 = True):\n",
    "    \"\"\"Splitting array in bins for the first exposure,\n",
    "    and compute the average (or not)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    exposure: numpy.array (3-dimensions) [raw, channels, trial]\n",
    "        Array containing data from an exposure.\n",
    "\n",
    "    indice_exposure: numpy.array (1-dimension)\n",
    "        Array containing indice when occurs the S2\n",
    "\n",
    "    average: bool \n",
    "        Control option to future analyses without average\n",
    "    Returns\n",
    "    -------\n",
    "    agg_per_trial: np.array\n",
    "    \n",
    "    \"\"\"\n",
    "    # Cutting values before the 0. -> y X 64 X 64 X 120\n",
    "    # y X 64 X 64 X 120\n",
    "    # 64 x (y * 64 tamanho) * trial\n",
    "    # 64 x 64 x (y * trial)\n",
    "    # 64 \n",
    "    \n",
    "    zero = 39\n",
    "\n",
    "    # Getting 2000 ms.\n",
    "    exposure_without_zero = exposure[zero:]\n",
    "\n",
    "    # Accumulator variable\n",
    "    agg_per_trial = []\n",
    "\n",
    "    for trial, ind_s2 in list(zip(exposure_without_zero.T, indice_exposure)):\n",
    "        \n",
    "        if last_32:\n",
    "                       \n",
    "            points_32 = trial.T[int(ind_s2-32):int(ind_s2)]\n",
    "        else:\n",
    "            points_32 = array(list(mit.windowed(trial.T, n=64)))\n",
    "            \n",
    "        #import pdb; pdb.set_trace()\n",
    "            \n",
    "        if(use_average):\n",
    "\n",
    "            agg_per_trial.append(average(points_32, axis=0).reshape(-1))\n",
    "\n",
    "        else:\n",
    "\n",
    "            agg_per_trial.append(points_32)\n",
    "    # ---------------------------------------------------------------------\n",
    "\n",
    "    return array(agg_per_trial).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:26:20.690541Z",
     "start_time": "2020-09-01T13:25:46.630Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_last_125ms_autoenconder(data, indices):\n",
    "    return get_last_125ms(data, indices, use_average=False)\n",
    "def get_last_125ms_autoenconder_w(data, indices):\n",
    "    return get_last_125ms(data, indices, use_average=False, last_32=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:26:20.692395Z",
     "start_time": "2020-09-01T13:25:46.635Z"
    }
   },
   "outputs": [],
   "source": [
    "import more_itertools as mit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:26:20.694240Z",
     "start_time": "2020-09-01T13:25:46.640Z"
    }
   },
   "outputs": [],
   "source": [
    "shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:26:20.696096Z",
     "start_time": "2020-09-01T13:25:46.644Z"
    }
   },
   "outputs": [],
   "source": [
    "aud_1_autoenconder = list(\n",
    "    map(get_last_125ms_autoenconder, aud_1, indice_s2_aud))\n",
    "aud_2_autoenconder = list(\n",
    "    map(get_last_125ms_autoenconder, aud_2, indice_s2_aud))\n",
    "\n",
    "\n",
    "vis_1_autoenconder = list(\n",
    "    map(get_last_125ms_autoenconder, vis_1, indice_s2_vis))\n",
    "vis_2_autoenconder = list(\n",
    "    map(get_last_125ms_autoenconder, vis_2, indice_s2_vis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:26:20.697951Z",
     "start_time": "2020-09-01T13:25:46.648Z"
    }
   },
   "outputs": [],
   "source": [
    "df_aud_1_autoenconder = to_DataFrame_autoenconder(\n",
    "    aud_1_autoenconder, classes_aud, CHANNEL_NAMES)\n",
    "df_aud_2_autoenconder = to_DataFrame_autoenconder(\n",
    "    aud_2_autoenconder, classes_aud, CHANNEL_NAMES)\n",
    "\n",
    "df_vis_1_autoenconder = to_DataFrame_autoenconder(\n",
    "    vis_1_autoenconder, classes_vis, CHANNEL_NAMES)\n",
    "df_vis_2_autoenconder = to_DataFrame_autoenconder(\n",
    "    vis_2_autoenconder, classes_vis, CHANNEL_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:26:20.699804Z",
     "start_time": "2020-09-01T13:25:46.654Z"
    }
   },
   "outputs": [],
   "source": [
    "df_aud_1_autoenconder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:26:20.701663Z",
     "start_time": "2020-09-01T13:25:46.658Z"
    }
   },
   "outputs": [],
   "source": [
    "df_aud_1_autoenconder.to_csv(\n",
    "    \"../data/processed/auditory_exposure_1_autoenconder.csv\", index=None)\n",
    "df_aud_2_autoenconder.to_csv(\n",
    "    \"../data/processed/auditory_exposure_2_autoenconder.csv\", index=None)\n",
    "\n",
    "\n",
    "df_vis_1_autoenconder.to_csv(\n",
    "    \"../data/processed/visual_exposure_1_autoenconder.csv\", index=None)\n",
    "df_vis_2_autoenconder.to_csv(\n",
    "    \"../data/processed/visual_exposure_2_autoenconder.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:26:20.703521Z",
     "start_time": "2020-09-01T13:25:46.663Z"
    }
   },
   "outputs": [],
   "source": [
    "vis_1_autoenconder_w = list(\n",
    "    map(get_last_125ms_autoenconder_w, vis_1, indice_s2_vis))\n",
    "vis_2_autoenconder_w = list(\n",
    "    map(get_last_125ms_autoenconder_w, vis_2, indice_s2_vis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:26:20.705363Z",
     "start_time": "2020-09-01T13:25:46.669Z"
    }
   },
   "outputs": [],
   "source": [
    "aud_1_autoenconder_w = list(\n",
    "    map(get_last_125ms_autoenconder_w, aud_1, indice_s2_aud))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:26:20.707139Z",
     "start_time": "2020-09-01T13:25:46.673Z"
    }
   },
   "outputs": [],
   "source": [
    "aud_2_autoenconder_w = list(\n",
    "    map(get_last_125ms_autoenconder_w, aud_2, indice_s2_aud))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:26:20.707626Z",
     "start_time": "2020-09-01T13:25:46.676Z"
    }
   },
   "outputs": [],
   "source": [
    "vis_1_all = np.concatenate(np.concatenate(vis_1.transpose([3,1,0,2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:26:20.708080Z",
     "start_time": "2020-09-01T13:25:46.681Z"
    }
   },
   "outputs": [],
   "source": [
    "vis_2_all = np.concatenate(np.concatenate(vis_2.transpose([3,1,0,2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:26:20.708574Z",
     "start_time": "2020-09-01T13:25:46.685Z"
    }
   },
   "outputs": [],
   "source": [
    "aud_1_all = np.concatenate(np.concatenate(aud_1.transpose([3,1,0,2])))\n",
    "aud_2_all = np.concatenate(np.concatenate(aud_2.transpose([3,1,0,2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:26:20.709010Z",
     "start_time": "2020-09-01T13:25:46.689Z"
    }
   },
   "outputs": [],
   "source": [
    "vis_1_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:26:20.709537Z",
     "start_time": "2020-09-01T13:25:46.694Z"
    }
   },
   "outputs": [],
   "source": [
    "data = np.array(aud_1_autoenconder_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:26:20.710092Z",
     "start_time": "2020-09-01T13:25:46.698Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from xarray import DataArray\n",
    "from pandas import DataFrame, merge\n",
    "from numpy import concatenate\n",
    "from scipy.stats import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:26:20.710457Z",
     "start_time": "2020-09-01T13:25:46.703Z"
    }
   },
   "outputs": [],
   "source": [
    "x_array = DataArray(data)\n",
    "x_array = x_array.rename({'dim_0': 'people','dim_1': 'channel','dim_2':'time','dim_3':'windows', 'dim_4':'trial'})\n",
    "x_array = x_array.transpose('people', 'trial', 'channel','time','windows')\n",
    "x_array.to_dataframe('channel').unstack()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:26:20.710986Z",
     "start_time": "2020-09-01T13:25:46.706Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_DataFrame_autoenconder_w(data, CHANNEL_NAMES):\n",
    "    '''\n",
    "    TO-DO\n",
    "    \n",
    "    '''\n",
    "    x_array = DataArray(data)\n",
    "    x_array = x_array.rename({'dim_0': 'people','dim_1': 'channel','dim_2':'time','dim_3':'windows', 'dim_4':'trial'})\n",
    "    x_array = x_array.transpose('people', 'trial', 'channel','time','windows')\n",
    "    x_array.to_dataframe('channel').unstack()\n",
    "\n",
    "    df = x_array.to_dataframe('time').unstack()\n",
    "\n",
    "    df_classe =  DataFrame(classe.stack()).reset_index()\n",
    "    df_classe.columns = ['people','trial','group']\n",
    "    df_v = df_classe.merge(df,on=['people','trial'], validate='one_to_many',how='outer')\n",
    "    df_v['channel'] = df.reset_index()['channel']\n",
    "\n",
    "    df_v['channel'].replace(dict(zip(list(range(64)),CHANNEL_NAMES)), inplace=True)\n",
    "\n",
    "    time_legend = ['time '+str(i) for i in range(32)]\n",
    "\n",
    "    df_v.columns = ['people',  'trial', 'group']+time_legend+['channel']\n",
    "\n",
    "    df_v = df_v[['people',  'trial', 'group','channel']+time_legend]\n",
    "\n",
    "    df_v = df_v[~((df_v['channel'] =='HEOG') | (df_v['channel'] =='VEOG'))]\n",
    "\n",
    "\n",
    "    return df_v\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": "0",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "691.75px",
    "left": "57px",
    "top": "111.133px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

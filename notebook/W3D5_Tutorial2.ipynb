{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "\n",
    "# Autoencoder \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "---\n",
    "# Tutorial Objectives\n",
    "\n",
    "## Architecture\n",
    "\n",
    "![Deep ANN autoencoder](https://github.com/mpbrigham/colaboratory-figures/raw/master/nma/autoencoders/ae-ann-3h.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T13:26:50.676313Z",
     "start_time": "2020-08-11T13:26:49.376670Z"
    },
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "colab_type": "code",
    "outputId": "12235495-f9b5-4209-c835-459563c72473"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video available at https://youtube.com/watch?v=pgkrU9UqXiU\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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\n",
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"854\"\n",
       "            height=\"480\"\n",
       "            src=\"https://www.youtube.com/embed/pgkrU9UqXiU?fs=1\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x7fc29c100828>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @title Video 1: Extensions\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"pgkrU9UqXiU\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "---\n",
    "# Setup\n",
    "Please execute the cell(s) below to initialize the notebook environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T19:57:48.009617Z",
     "start_time": "2020-08-18T19:57:35.244470Z"
    },
    "cellView": "both",
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T19:46:48.940817Z",
     "start_time": "2020-08-18T19:45:47.327047Z"
    },
    "cellView": "form",
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Keyring is skipped due to an exception: Failed to unlock the collection!\u001b[0m\n",
      "\u001b[33mWARNING: Keyring is skipped due to an exception: Failed to unlock the collection!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# @title Figure settings\n",
    "#!pip install plotly --quiet\n",
    "import plotly.graph_objects as go\n",
    "from plotly.colors import qualitative\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T19:58:50.740133Z",
     "start_time": "2020-08-18T19:58:50.713468Z"
    },
    "cellView": "form",
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# @title Helper functions\n",
    "\n",
    "\n",
    "def downloadMNIST():\n",
    "  \"\"\"\n",
    "  Download MNIST dataset and transform it to torch.Tensor\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    x_train : training images (torch.Tensor) (60000, 28, 28)\n",
    "    x_test  : test images (torch.Tensor) (10000, 28, 28)\n",
    "    y_train : training labels (torch.Tensor) (60000, )\n",
    "    y_train : test labels (torch.Tensor) (10000, )\n",
    "  \"\"\"\n",
    "  X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "  # Trunk the data\n",
    "  n_train = 60000\n",
    "  n_test = 10000\n",
    "\n",
    "  train_idx = np.arange(0, n_train)\n",
    "  test_idx = np.arange(n_train, n_train + n_test)\n",
    "\n",
    "  x_train, y_train = X[train_idx], y[train_idx]\n",
    "  x_test, y_test = X[test_idx], y[test_idx]\n",
    "\n",
    "  # Transform np.ndarrays to torch.Tensor\n",
    "  x_train = torch.from_numpy(np.reshape(x_train,\n",
    "                                        (len(x_train),\n",
    "                                         28, 28)).astype(np.float32))\n",
    "  x_test = torch.from_numpy(np.reshape(x_test,\n",
    "                                       (len(x_test),\n",
    "                                        28, 28)).astype(np.float32))\n",
    "\n",
    "  y_train = torch.from_numpy(y_train.astype(int))\n",
    "  y_test = torch.from_numpy(y_test.astype(int))\n",
    "\n",
    "  return (x_train, y_train, x_test, y_test)\n",
    "\n",
    "\n",
    "def init_weights_kaiming_uniform(layer):\n",
    "  \"\"\"\n",
    "  Initializes weights from linear PyTorch layer\n",
    "  with kaiming uniform distribution.\n",
    "\n",
    "  Args:\n",
    "    layer (torch.Module)\n",
    "        Pytorch layer\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "  # check for linear PyTorch layer\n",
    "  if isinstance(layer, nn.Linear):\n",
    "    # initialize weights with kaiming uniform distribution\n",
    "    nn.init.kaiming_uniform_(layer.weight.data)\n",
    "\n",
    "\n",
    "def init_weights_kaiming_normal(layer):\n",
    "  \"\"\"\n",
    "  Initializes weights from linear PyTorch layer\n",
    "  with kaiming normal distribution.\n",
    "\n",
    "  Args:\n",
    "    layer (torch.Module)\n",
    "        Pytorch layer\n",
    "\n",
    "  Returns:nossa\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "  # check for linear PyTorch layer\n",
    "  if isinstance(layer, nn.Linear):\n",
    "    # initialize weights with kaiming normal distribution\n",
    "    nn.init.kaiming_normal_(layer.weight.data)\n",
    "\n",
    "\n",
    "def get_layer_weights(layer):\n",
    "  \"\"\"\n",
    "  Retrieves learnable parameters from PyTorch layer.\n",
    "\n",
    "  Args:\n",
    "    layer (torch.Module)\n",
    "        Pytorch layer\n",
    "\n",
    "  Returns:\n",
    "    list with learnable parameters\n",
    "  \"\"\"\n",
    "  # initialize ounossatput list\n",
    "  weights = []\n",
    "\n",
    "  # check whether layer has learnable parameters\n",
    "  if layer.parameters():\n",
    "    # copy numpy array representation of each set of learnable parameters\n",
    "    for item in layer.parameters():\n",
    "      weights.append(item.detach().numpy())\n",
    "\n",
    "  return weights\n",
    "\n",
    "\n",
    "def print_parameter_count(net):\n",
    "  \"\"\"\n",
    "  Prints count of learnable parameters per layer from PyTorch network.\n",
    "\n",
    "  Args:\n",
    "    net (torch.Sequential)\n",
    "        Pytorch network\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "\n",
    "  params_n = 0\n",
    "\n",
    "  # loop all layers in network\n",
    "  for layer_idx, layer in enumerate(net):\n",
    "\n",
    "    # retrieve learnable parameters\n",
    "    weights = get_layer_weights(layer)\n",
    "    params_layer_n = 0\n",
    "\n",
    "    # loop list of learnable parameters and count them\n",
    "    for params in weights:\n",
    "      params_layer_n += params.size\n",
    "\n",
    "    params_n += params_layer_n\n",
    "    print(f'{layer_idx}\\t {params_layer_n}\\t {layer}')\n",
    "\n",
    "  print(f'\\nTotal:\\t {params_n}')\n",
    "\n",
    "\n",
    "def eval_mse(y_pred, y_true):\n",
    "  \"\"\"\n",
    "  Evaluates mean square error (MSE) between y_pred and y_true\n",
    "\n",
    "  Args:\n",
    "    y_pred (torch.Tensor)\n",
    "        prediction samples\n",
    "\n",
    "    v (numpy array of floats)\n",
    "        ground truth samples\n",
    "\n",
    "  Returns:\n",
    "    MSE(y_pred, y_true)\n",
    "  \"\"\"\n",
    "\n",
    "  with torch.no_grad():\n",
    "      criterion = nn.MSELoss()\n",
    "      loss = criterion(y_pred, y_true)\n",
    "\n",
    "  return float(loss)\n",
    "\n",
    "\n",
    "def eval_bce(y_pred, y_true):\n",
    "  \"\"\"\n",
    "  Evaluates binary cross-entropy (BCE) between y_pred and y_true\n",
    "\n",
    "  Args:\n",
    "    y_pred (torch.Tensor)\n",
    "        prediction samples\n",
    "\n",
    "    v (numpy array of floats)\n",
    "        ground truth samples\n",
    "\n",
    "  Returns:\n",
    "    BCE(y_pred, y_true)\n",
    "  \"\"\"\n",
    "\n",
    "  with torch.no_grad():\n",
    "    criterion = nn.BCELoss()\n",
    "    loss = criterion(y_pred, y_true)\n",
    "\n",
    "  return float(loss)\n",
    "\n",
    "\n",
    "def plot_row(images, show_n=10, image_shape=None):\n",
    "  \"\"\"\n",
    "  Plots rows of images from list of iterables (iterables: list, numpy array\n",
    "  or torch.Tensor). Also accepts single iterable.\n",
    "  Randomly selects images in each list element if item count > show_n.\n",
    "\n",
    "  Args:\n",
    "    images (iterable or list of iterables)\n",
    "        single iterable with images, or list of iterables\n",
    "\n",
    "    show_n (integer)\n",
    "        maximum number of images per row\n",
    "\n",
    "    image_shape (tuple or list)\n",
    "        original shape of image if vectorized form\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "\n",
    "  if not isinstance(images, (list, tuple)):\n",
    "    images = [images]\n",
    "\n",
    "  for items_idx, items in enumerate(images):\n",
    "\n",
    "    items = np.array(items)\n",
    "    if items.ndim == 1:\n",
    "      items = np.expand_dims(items, axis=0)\n",
    "\n",
    "    if len(items) > show_n:\n",
    "      selected = np.random.choice(len(items), show_n, replace=False)\n",
    "      items = items[selected]\n",
    "\n",
    "    if image_shape is not None:\n",
    "      items = items.reshape([-1]+list(image_shape))\n",
    "\n",
    "    plt.figure(figsize=(len(items) * 1.5, 2))\n",
    "    for image_idx, image in enumerate(items):\n",
    "\n",
    "      plt.subplot(1, len(items), image_idx + 1)\n",
    "      plt.imshow(image, cmap='gray', vmin=image.min(), vmax=image.max())\n",
    "      plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "def to_s2(u):\n",
    "  \"\"\"\n",
    "  Projects 3D coordinates to spherical coordinates (theta, phi) surface of\n",
    "  unit sphere S2.\n",
    "  theta: [0, pi]\n",
    "  phi: [-pi, pi]\n",
    "\n",
    "  Args:\n",
    "    u (list, numpy array or torch.Tensor of floats)\n",
    "        3D coordinates\n",
    "\n",
    "  Returns:\n",
    "    Sperical coordinates (theta, phi) on surface of unit sphere S2.\n",
    "  \"\"\"\n",
    "\n",
    "  x, y, z = (u[:, 0], u[:, 1], u[:, 2])\n",
    "  r = np.sqrt(x**2 + y**2 + z**2)\n",
    "  theta = np.arccos(z / r)\n",
    "  phi = np.arctan2(x, y)\n",
    "\n",
    "  return np.array([theta, phi]).T\n",
    "\n",
    "\n",
    "def to_u3(s):\n",
    "  \"\"\"\n",
    "  Converts from 2D coordinates on surface of unit sphere S2 to 3D coordinates\n",
    "  (on surface of S2), i.e. (theta, phi) ---> (1, theta, phi).\n",
    "\n",
    "  Args:\n",
    "    s (list, numpy array or torch.Tensor of floats)\n",
    "        2D coordinates on unit sphere S_2\n",
    "\n",
    "  Returns:\n",
    "    3D coordinates on surface of unit sphere S_2\n",
    "  \"\"\"\n",
    "\n",
    "  theta, phi = (s[:, 0], s[:, 1])\n",
    "  x = np.sin(theta) * np.sin(phi)\n",
    "  y = np.sin(theta) * np.cos(phi)\n",
    "  z = np.cos(theta)\n",
    "\n",
    "  return np.array([x, y, z]).T\n",
    "\n",
    "\n",
    "def xy_lim(x):\n",
    "  \"\"\"\n",
    "  Return arguments for plt.xlim and plt.ylim calculated from minimum\n",
    "  and maximum of x.\n",
    "\n",
    "  Args:\n",
    "    x (list, numpy array or torch.Tensor of floats)\n",
    "        data to be plotted\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "\n",
    "  x_min = np.min(x, axis=0)\n",
    "  x_max = np.max(x, axis=0)\n",
    "\n",
    "  x_min = x_min - np.abs(x_max - x_min) * 0.05 - np.finfo(float).eps\n",
    "  x_max = x_max + np.abs(x_max - x_min) * 0.05 + np.finfo(float).eps\n",
    "\n",
    "  return [x_min[0], x_max[0]], [x_min[1], x_max[1]]\n",
    "\n",
    "\n",
    "def plot_generative(x, decoder_fn, image_shape, n_row=16, s2=False):\n",
    "  \"\"\"\n",
    "  Plots images reconstructed by decoder_fn from a 2D grid in\n",
    "  latent space that is determined by minimum and maximum values in x.\n",
    "\n",
    "  Args:\n",
    "    x (list, numpy array or torch.Tensor of floats)\n",
    "        2D or 3D coordinates in latent space\n",
    "\n",
    "    decoder_fn (integer)\n",
    "        function returning vectorized images from 2D latent space coordinates\n",
    "\n",
    "    image_shape (tuple or list)\n",
    "        original shape of image\n",
    "\n",
    "    n_row (integer)\n",
    "        number of rows in grid\n",
    "\n",
    "    s2 (boolean)\n",
    "        convert 3D coordinates (x, y, z) to spherical coordinates (theta, phi)\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "\n",
    "  if s2:\n",
    "    x = to_s2(np.array(x))\n",
    "\n",
    "  xlim, ylim = xy_lim(np.array(x))\n",
    "\n",
    "  dx = (xlim[1] - xlim[0]) / n_row\n",
    "  grid = [np.linspace(ylim[0] + dx / 2, ylim[1] - dx / 2, n_row),\n",
    "          np.linspace(xlim[0] + dx / 2, xlim[1] - dx / 2, n_row)]\n",
    "\n",
    "  canvas = np.zeros((image_shape[0] * n_row, image_shape[1] * n_row))\n",
    "\n",
    "  cmap = plt.get_cmap('gray')\n",
    "\n",
    "  for j, latent_y in enumerate(grid[0][::-1]):\n",
    "    for i, latent_x in enumerate(grid[1]):\n",
    "\n",
    "      latent = np.array([[latent_x, latent_y]], dtype=np.float32)\n",
    "\n",
    "      if s2:\n",
    "        latent = to_u3(latent)\n",
    "\n",
    "      with torch.no_grad():\n",
    "        x_decoded = decoder_fn(torch.from_numpy(latent))\n",
    "\n",
    "      x_decoded = x_decoded.reshape(image_shape)\n",
    "\n",
    "      canvas[j * image_shape[0]: (j + 1) * image_shape[0],\n",
    "             i * image_shape[1]: (i + 1) * image_shape[1]] = x_decoded\n",
    "\n",
    "  plt.imshow(canvas, cmap=cmap, vmin=canvas.min(), vmax=canvas.max())\n",
    "  plt.axis('off')\n",
    "\n",
    "\n",
    "def plot_latent(x, y, show_n=500, s2=False, fontdict=None, xy_labels=None):\n",
    "  \"\"\"\n",
    "  Plots digit class of each sample in 2D latent space coordinates.\n",
    "\n",
    "  Args:\n",
    "    x (list, numpy array or torch.Tensor of floats)\n",
    "        2D coordinates in latent space\n",
    "\n",
    "    y (list, numpy array or torch.Tensor of floats)\n",
    "        digit class of each sample\n",
    "\n",
    "    n_row (integer)\n",
    "        number of samples\n",
    "\n",
    "    s2 (boolean)\n",
    "        convert 3D coordinates (x, y, z) to spherical coordinates (theta, phi)\n",
    "\n",
    "    fontdict (dictionary)\n",
    "        style option for plt.text\n",
    "\n",
    "    xy_labels (list)\n",
    "        optional list with [xlabel, ylabel]\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "\n",
    "  if fontdict is None:\n",
    "    fontdict = {'weight': 'bold', 'size': 12}\n",
    "\n",
    "  if s2:\n",
    "    x = to_s2(np.array(x))\n",
    "\n",
    "  cmap = plt.get_cmap('tab10')\n",
    "\n",
    "  if len(x) > show_n:\n",
    "    selected = np.random.choice(len(x), show_n, replace=False)\n",
    "    x = x[selected]\n",
    "    y = y[selected]\n",
    "\n",
    "  for my_x, my_y in zip(x, y):\n",
    "    plt.text(my_x[0], my_x[1], str(int(my_y)),\n",
    "             color=cmap(int(my_y) / 10.),\n",
    "             fontdict=fontdict,\n",
    "             horizontalalignment='center',\n",
    "             verticalalignment='center',\n",
    "             alpha=0.8)\n",
    "\n",
    "  xlim, ylim = xy_lim(np.array(x))\n",
    "  plt.xlim(xlim)\n",
    "  plt.ylim(ylim)\n",
    "\n",
    "  if s2:\n",
    "    if xy_labels is None:\n",
    "      xy_labels = [r'$\\varphi$', r'$\\theta$']\n",
    "\n",
    "    plt.xticks(np.arange(0, np.pi + np.pi / 6, np.pi / 6),\n",
    "               ['0', '$\\pi/6$', '$\\pi/3$', '$\\pi/2$',\n",
    "                '$2\\pi/3$', '$5\\pi/6$', '$\\pi$'])\n",
    "    plt.yticks(np.arange(-np.pi, np.pi + np.pi / 3, np.pi / 3),\n",
    "               ['$-\\pi$', '$-2\\pi/3$', '$-\\pi/3$', '0',\n",
    "                '$\\pi/3$', '$2\\pi/3$', '$\\pi$'])\n",
    "\n",
    "  if xy_labels is None:\n",
    "    xy_labels = ['$Z_1$', '$Z_2$']\n",
    "\n",
    "  plt.xlabel(xy_labels[0])\n",
    "  plt.ylabel(xy_labels[1])\n",
    "\n",
    "\n",
    "def plot_latent_generative(x, y, decoder_fn, image_shape, s2=False,\n",
    "                           title=None, xy_labels=None):\n",
    "  \"\"\"\n",
    "  Two horizontal subplots generated with encoder map and decoder grid.\n",
    "\n",
    "  Args:\n",
    "    x (list, numpy array or torch.Tensor of floats)\n",
    "        2D coordinates in latent space\n",
    "\n",
    "    y (list, numpy array or torch.Tensor of floats)\n",
    "        digit class of each sample\n",
    "\n",
    "    decoder_fn (integer)\n",
    "        function returning vectorized images from 2D latent space coordinates\n",
    "\n",
    "    image_shape (tuple or list)\n",
    "        original shape of image\n",
    "\n",
    "    s2 (boolean)\n",
    "        convert 3D coordinates (x, y, z) to spherical coordinates (theta, phi)\n",
    "\n",
    "    title (string)\n",
    "        plot title\n",
    "\n",
    "    xy_labels (list)\n",
    "        optional list with [xlabel, ylabel]\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "\n",
    "  fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "  if title is not None:\n",
    "    fig.suptitle(title, y=1.05)\n",
    "\n",
    "  ax = fig.add_subplot(121)\n",
    "  ax.set_title('Encoder map', y=1.05)\n",
    "  plot_latent(x, y, s2=s2, xy_labels=xy_labels)\n",
    "\n",
    "  ax = fig.add_subplot(122)\n",
    "  ax.set_title('Decoder grid', y=1.05)\n",
    "  plot_generative(x, decoder_fn, image_shape, s2=s2)\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def plot_latent_3d(my_x, my_y, show_text=True, show_n=500):\n",
    "  \"\"\"\n",
    "  Plot digit class or marker in 3D latent space coordinates.\n",
    "\n",
    "  Args:\n",
    "    my_x (list, numpy array or torch.Tensor of floats)\n",
    "        2D coordinates in latent space\n",
    "\n",
    "    my_y (list, numpy array or torch.Tensor of floats)\n",
    "        digit class of each sample\n",
    "\n",
    "    show_text (boolean)\n",
    "        whether to show text\n",
    "\n",
    "    image_shape (tuple or list)\n",
    "        original shape of image\n",
    "\n",
    "    s2 (boolean)\n",
    "        convert 3D coordinates (x, y, z) to spherical coordinates (theta, phi)\n",
    "\n",
    "    title (string)\n",
    "        plot title\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "\n",
    "  layout = {'margin': {'l': 0, 'r': 0, 'b': 0, 't': 0},\n",
    "            'scene': {'xaxis': {'showspikes': False,\n",
    "                                'title': 'z1'},\n",
    "                      'yaxis': {'showspikes': False,\n",
    "                                'title': 'z2'},\n",
    "                      'zaxis': {'showspikes': False,\n",
    "                                'title': 'z3'}}\n",
    "            }\n",
    "\n",
    "  selected_idx = np.random.choice(len(my_x), show_n, replace=False)\n",
    "\n",
    "  colors = [qualitative.T10[idx] for idx in my_y[selected_idx]]\n",
    "\n",
    "  x = my_x[selected_idx, 0]\n",
    "  y = my_x[selected_idx, 1]\n",
    "  z = my_x[selected_idx, 2]\n",
    "\n",
    "  text = my_y[selected_idx]\n",
    "\n",
    "  if show_text:\n",
    "\n",
    "    trace = go.Scatter3d(x=x, y=y, z=z, text=text,\n",
    "                         mode='text',\n",
    "                         textfont={'color': colors, 'size': 12}\n",
    "                         )\n",
    "\n",
    "    layout['hovermode'] = False\n",
    "\n",
    "  else:\n",
    "\n",
    "    trace = go.Scatter3d(x=x, y=y, z=z, text=text,\n",
    "                         hoverinfo='text', mode='markers',\n",
    "                         marker={'size': 5, 'color': colors, 'opacity': 0.8}\n",
    "                         )\n",
    "\n",
    "  fig = go.Figure(data=trace, layout=layout)\n",
    "\n",
    "  fig.show()\n",
    "\n",
    "\n",
    "def runSGD(net, input_train, input_test, criterion='bce',\n",
    "           n_epochs=10, batch_size=32, verbose=False):\n",
    "  \"\"\"\n",
    "  Trains autoencoder network with stochastic gradient descent with Adam\n",
    "  optimizer and loss criterion. Train samples are shuffled, and loss is\n",
    "  displayed at the end of each opoch for both MSE and BCE. Plots training loss\n",
    "  at each minibatch (maximum of 500 randomly selected values).\n",
    "\n",
    "  Args:\n",
    "    net (torch network)\n",
    "        ANN object (nn.Module)\n",
    "\n",
    "    input_train (torch.Tensor)\n",
    "        vectorized input images from train set\n",
    "\n",
    "    input_test (torch.Tensor)\n",
    "        vectorized input images from test set\n",
    "\n",
    "    criterion (string)\n",
    "        train loss: 'bce' or 'mse'\n",
    "\n",
    "    n_epochs (boolean)\n",
    "        number of full iterations of training data\n",
    "\n",
    "    batch_size (integer)\n",
    "        number of element in mini-batches\n",
    "\n",
    "    verbose (boolean)\n",
    "        print final loss\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "\n",
    "  # Initialize loss function\n",
    "  if criterion == 'mse':\n",
    "    loss_fn = nn.MSELoss()\n",
    "  elif criterion == 'bce':\n",
    "    loss_fn = nn.BCELoss()\n",
    "  else:\n",
    "    print('Please specify either \"mse\" or \"bce\" for loss criterion')\n",
    "\n",
    "  # Initialize SGD optimizer\n",
    "  optimizer = optim.Adam(net.parameters())\n",
    "\n",
    "  # Placeholder for loss\n",
    "  track_loss = []\n",
    "\n",
    "  print('Epoch', '\\t', 'Loss train', '\\t', 'Loss test')\n",
    "  for i in range(n_epochs):\n",
    "\n",
    "    shuffle_idx = np.random.permutation(len(input_train))\n",
    "    batches = torch.split(input_train[shuffle_idx], batch_size)\n",
    "\n",
    "    for batch in batches:\n",
    "\n",
    "      output_train = net(batch)\n",
    "      loss = loss_fn(output_train, batch)\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # Keep track of loss at each epoch\n",
    "      track_loss += [float(loss)]\n",
    "\n",
    "    loss_epoch = f'{i+1}/{n_epochs}'\n",
    "    with torch.no_grad():\n",
    "      output_train = net(input_train)\n",
    "      loss_train = loss_fn(output_train, input_train)\n",
    "      loss_epoch += f'\\t {loss_train:.4f}'\n",
    "\n",
    "      output_test = net(input_test)\n",
    "      loss_test = loss_fn(output_test, input_test)\n",
    "      loss_epoch += f'\\t\\t {loss_test:.4f}'\n",
    "\n",
    "    print(loss_epoch)\n",
    "\n",
    "  if verbose:\n",
    "    # Print loss\n",
    "    loss_mse = f'\\nMSE\\t {eval_mse(output_train, input_train):0.4f}'\n",
    "    loss_mse += f'\\t\\t {eval_mse(output_test, input_test):0.4f}'\n",
    "    print(loss_mse)\n",
    "\n",
    "    loss_bce = f'BCE\\t {eval_bce(output_train, input_train):0.4f}'\n",
    "    loss_bce += f'\\t\\t {eval_bce(output_test, input_test):0.4f}'\n",
    "    print(loss_bce)\n",
    "\n",
    "  # Plot loss\n",
    "  step = int(np.ceil(len(track_loss) / 500))\n",
    "  x_range = np.arange(0, len(track_loss), step)\n",
    "  plt.figure()\n",
    "  plt.plot(x_range, track_loss[::step], 'C0')\n",
    "  plt.xlabel('Iterations')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.xlim([0, None])\n",
    "  plt.ylim([0, None])\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "class NormalizeLayer(nn.Module):\n",
    "  \"\"\"\n",
    "  pyTorch layer (nn.Module) that normalizes activations by their L2 norm.\n",
    "\n",
    "  Args:\n",
    "      None.\n",
    "\n",
    "  Returns:\n",
    "      Object inherited from nn.Module class.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "  def forward(self, x):\n",
    "    return nn.functional.normalize(x, p=2, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "---\n",
    "# Section 1: Download  and prepare MNIST dataset\n",
    "We use the helper function `downloadMNIST` to download the dataset and transform it into `torch.Tensor` and assign train and test sets to (`x_train`, `y_train`) and (`x_test`, `y_test`).\n",
    "\n",
    "The variable `input_size` stores the length of *vectorized* versions of the images `input_train` and `input_test` for training and test images.\n",
    "\n",
    "**Instructions:**\n",
    "* Please execute the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T18:07:41.095823Z",
     "start_time": "2020-08-18T18:07:18.109524Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "outputId": "8124b210-b0a2-49b4-bc8c-7cfb647f675e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape image \t \t torch.Size([28, 28])\n",
      "shape input_train \t torch.Size([60000, 784])\n",
      "shape input_test \t torch.Size([10000, 784])\n"
     ]
    }
   ],
   "source": [
    "# Download MNIST\n",
    "x_train, y_train, x_test, y_test = downloadMNIST()\n",
    "\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "image_shape = x_train.shape[1:]\n",
    "\n",
    "input_size = np.prod(image_shape)\n",
    "\n",
    "input_train = x_train.reshape([-1, input_size])\n",
    "input_test = x_test.reshape([-1, input_size])\n",
    "\n",
    "test_selected_idx = np.random.choice(len(x_test), 10, replace=False)\n",
    "train_selected_idx = np.random.choice(len(x_train), 10, replace=False)\n",
    "\n",
    "print(f'shape image \\t \\t {image_shape}')\n",
    "print(f'shape input_train \\t {input_train.shape}')\n",
    "print(f'shape input_test \\t {input_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T19:59:00.164313Z",
     "start_time": "2020-08-18T19:59:00.158738Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"../src/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T19:59:06.373959Z",
     "start_time": "2020-08-18T19:59:01.728175Z"
    }
   },
   "outputs": [],
   "source": [
    "from file import read_file\n",
    "import more_itertools as mit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T19:59:06.384644Z",
     "start_time": "2020-08-18T19:59:06.378902Z"
    }
   },
   "outputs": [],
   "source": [
    "PATH_AUD = '../data/raw/aud'\n",
    "PATH_VIS = '../data/raw/vis'\n",
    "PATH_INFO = '../data/raw/info_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T19:59:25.533508Z",
     "start_time": "2020-08-18T19:59:06.489651Z"
    }
   },
   "outputs": [],
   "source": [
    "data_aud, data_vis, CHANNEL_NAMES = read_file(PATH_AUD, PATH_VIS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T19:59:25.766912Z",
     "start_time": "2020-08-18T19:59:25.534930Z"
    }
   },
   "outputs": [],
   "source": [
    "aud = np.concatenate(data_aud.transpose([1,3,0,2]),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T20:35:50.959530Z",
     "start_time": "2020-08-18T20:32:53.379676Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-7ec2fc354b6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_aud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.concatenate(data_aud.transpose([1,3,0,2]),0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T19:59:33.051096Z",
     "start_time": "2020-08-18T19:59:25.858923Z"
    }
   },
   "outputs": [],
   "source": [
    "aud_32 = np.array(list(mit.windowed(aud, n=32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T20:01:10.691348Z",
     "start_time": "2020-08-18T19:59:41.149913Z"
    }
   },
   "outputs": [],
   "source": [
    "#.astype(np.float64)\n",
    "\n",
    "por_canal = [torch.from_numpy(aud_.astype(np.float32)) for aud_ in aud_32.T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T19:47:26.124712Z",
     "start_time": "2020-08-18T19:47:25.281348Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-2.70801963e+00, -4.28758949e+00,  1.77966651e+00, ...,\n",
       "          6.05932743e-01,  6.07788277e+00,  3.86679581e+00],\n",
       "        [ 1.11246873e+01,  6.34083248e+00,  8.69902822e+00, ...,\n",
       "          2.12879110e+00, -4.24623566e+00,  5.79047556e+00],\n",
       "        [-1.03166227e+00,  2.46388221e+00, -3.46992845e+00, ...,\n",
       "         -2.85655304e+00, -7.12469715e-01, -2.33270002e+00],\n",
       "        ...,\n",
       "        [ 3.81860242e+00,  3.46653514e+00,  9.91841998e+00, ...,\n",
       "          6.40215308e-01,  2.80588423e+00,  1.08878596e+01],\n",
       "        [ 7.31461645e+00,  1.69159192e+01,  1.08772352e+01, ...,\n",
       "          6.48718250e-01,  1.11205732e+01,  1.92914216e+00],\n",
       "        [ 9.44899194e+00,  1.80464227e+01,  3.65042446e+00, ...,\n",
       "         -3.22483707e+00,  9.80562738e+00,  3.15087221e+00]],\n",
       "\n",
       "       [[ 1.11246873e+01,  6.34083248e+00,  8.69902822e+00, ...,\n",
       "          2.12879110e+00, -4.24623566e+00,  5.79047556e+00],\n",
       "        [-1.03166227e+00,  2.46388221e+00, -3.46992845e+00, ...,\n",
       "         -2.85655304e+00, -7.12469715e-01, -2.33270002e+00],\n",
       "        [ 8.80572947e+00,  8.75630261e+00,  1.68198567e-02, ...,\n",
       "         -3.46741152e+00,  5.92431342e+00,  9.09304623e-01],\n",
       "        ...,\n",
       "        [ 7.31461645e+00,  1.69159192e+01,  1.08772352e+01, ...,\n",
       "          6.48718250e-01,  1.11205732e+01,  1.92914216e+00],\n",
       "        [ 9.44899194e+00,  1.80464227e+01,  3.65042446e+00, ...,\n",
       "         -3.22483707e+00,  9.80562738e+00,  3.15087221e+00],\n",
       "        [-6.57668885e+00, -3.03825752e+00, -1.52863477e+00, ...,\n",
       "         -2.14254263e+00, -4.31474898e-01, -1.64756013e+00]],\n",
       "\n",
       "       [[-1.03166227e+00,  2.46388221e+00, -3.46992845e+00, ...,\n",
       "         -2.85655304e+00, -7.12469715e-01, -2.33270002e+00],\n",
       "        [ 8.80572947e+00,  8.75630261e+00,  1.68198567e-02, ...,\n",
       "         -3.46741152e+00,  5.92431342e+00,  9.09304623e-01],\n",
       "        [ 9.83610119e-01,  4.58848797e+00, -3.90786432e+00, ...,\n",
       "          2.08697969e+00,  1.35005783e+00,  9.94938741e+00],\n",
       "        ...,\n",
       "        [ 9.44899194e+00,  1.80464227e+01,  3.65042446e+00, ...,\n",
       "         -3.22483707e+00,  9.80562738e+00,  3.15087221e+00],\n",
       "        [-6.57668885e+00, -3.03825752e+00, -1.52863477e+00, ...,\n",
       "         -2.14254263e+00, -4.31474898e-01, -1.64756013e+00],\n",
       "        [ 3.03858582e+00, -2.28192039e-01, -1.03475918e+00, ...,\n",
       "          1.43713793e+01,  4.45602775e+00,  3.05448992e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-6.39863105e+00, -2.29960373e+00,  1.91171514e+01, ...,\n",
       "          7.91347894e+00,  2.31627988e+00, -4.59395804e+01],\n",
       "        [-6.05006006e+00, -1.04431930e+01,  5.45525016e-01, ...,\n",
       "          2.77421481e+00,  1.72174081e+00,  3.12174680e+01],\n",
       "        [-2.10693078e+01, -1.63663658e+01,  8.35407563e+00, ...,\n",
       "          2.11539179e+01,  7.87143023e-02,  2.22095497e+01],\n",
       "        ...,\n",
       "        [-8.05787904e+01, -3.54408649e+01, -4.73634422e+01, ...,\n",
       "         -2.16282480e+01, -7.93829664e+01, -4.04749816e+01],\n",
       "        [-1.22675570e+01, -1.55012197e+01, -1.28036483e+01, ...,\n",
       "         -1.39850221e+01, -9.12030949e+00, -2.64039437e+01],\n",
       "        [-4.70173755e+01, -5.69334186e+01, -4.28587740e+01, ...,\n",
       "         -7.31338207e+00, -1.09274617e+02, -1.99571307e+00]],\n",
       "\n",
       "       [[-6.05006006e+00, -1.04431930e+01,  5.45525016e-01, ...,\n",
       "          2.77421481e+00,  1.72174081e+00,  3.12174680e+01],\n",
       "        [-2.10693078e+01, -1.63663658e+01,  8.35407563e+00, ...,\n",
       "          2.11539179e+01,  7.87143023e-02,  2.22095497e+01],\n",
       "        [ 7.34485686e+00,  5.78898039e+00,  1.54989971e+01, ...,\n",
       "         -5.45529988e+00,  3.94833555e+00,  3.13259903e+00],\n",
       "        ...,\n",
       "        [-1.22675570e+01, -1.55012197e+01, -1.28036483e+01, ...,\n",
       "         -1.39850221e+01, -9.12030949e+00, -2.64039437e+01],\n",
       "        [-4.70173755e+01, -5.69334186e+01, -4.28587740e+01, ...,\n",
       "         -7.31338207e+00, -1.09274617e+02, -1.99571307e+00],\n",
       "        [-2.52925625e+01, -2.62997712e+01, -2.50228520e+01, ...,\n",
       "         -8.90434132e+00, -3.51359318e+01,  2.34205575e+00]],\n",
       "\n",
       "       [[-2.10693078e+01, -1.63663658e+01,  8.35407563e+00, ...,\n",
       "          2.11539179e+01,  7.87143023e-02,  2.22095497e+01],\n",
       "        [ 7.34485686e+00,  5.78898039e+00,  1.54989971e+01, ...,\n",
       "         -5.45529988e+00,  3.94833555e+00,  3.13259903e+00],\n",
       "        [ 9.10474121e+00,  5.11198411e+00, -4.07360776e+00, ...,\n",
       "         -8.17742279e+00,  1.53841700e-01, -1.75470632e+01],\n",
       "        ...,\n",
       "        [-4.70173755e+01, -5.69334186e+01, -4.28587740e+01, ...,\n",
       "         -7.31338207e+00, -1.09274617e+02, -1.99571307e+00],\n",
       "        [-2.52925625e+01, -2.62997712e+01, -2.50228520e+01, ...,\n",
       "         -8.90434132e+00, -3.51359318e+01,  2.34205575e+00],\n",
       "        [-1.51017177e+00, -5.91560373e+00, -4.93520462e+00, ...,\n",
       "          7.45613945e+00, -5.31521875e+00, -1.29440855e+01]]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "240*539*20, 64 -> (240*539*20)-31, 32, 64\n",
    "\n",
    "[(240*539*20)-31, 32], 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T20:36:17.157135Z",
     "start_time": "2020-08-18T20:36:17.008611Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2587169"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(240*539*20)-31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "---\n",
    "# Section 2: Deeper autoencoder (2D)\n",
    "The internal representation of shallow autoencoder with 2D latent space is similar to PCA, which shows that the autoencoder is not fully leveraging non-linear capabilities to model data. Adding capacity in terms of learnable parameters takes advantage of non-linear operations in encoding/decoding to capture non-linear patterns in data.\n",
    "\n",
    "Adding hidden layers enables us to introduce additional parameters, either layerwise or depthwise. The same amount $N$ of additional parameters can be added in a single layer or distributed among several layers. Adding several hidden layers reduces the compression/decompression ratio of each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Exercise 1: Build deeper autoencoder (2D)\n",
    "Implement this deeper version of the ANN autoencoder by adding four hidden layers. The number of units per layer in the encoder is the following:\n",
    "\n",
    "```\n",
    "784 -> 392 -> 64 -> 2\n",
    "```\n",
    "\n",
    "The shallow autoencoder has a compression ratio of **784:2 = 392:1**. The first additional hidden layer has a compression ratio of **2:1**,  followed by a hidden layer that sets the bottleneck compression ratio of **32:1**.\n",
    "\n",
    "The choice of hidden layer size aims to reduce the compression rate in the bottleneck layer while increasing the count of trainable parameters.  For example, if the compression rate of the first hidden layer doubles from **2:1** to **4:1**, the count of trainable parameters halves from 667K to 333K.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "This deep autoencoder's performance may be further improved by adding additional hidden layers and by increasing the count of trainable parameters in each layer. These improvements have a diminishing return due to challenges associated with training under high parameter count and depth. One option explored in the *Bonus* section is to add a first hidden layer with 2x - 3x the input size. This size increase results in millions of parameters at the cost of longer training time.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Weight initialization is particularly important in deep networks. The availability of large datasets and weight initialization likely drove the deep learning revolution of 2010. We'll implement Kaiming normal as follows:\n",
    "```\n",
    "model[:-2].apply(init_weights_kaiming_normal)\n",
    "```\n",
    "\n",
    "**Instructions:**\n",
    "* Add four additional layers and activation functions to the network\n",
    "* Adjust the definitions of `encoder` and `decoder`\n",
    "* Check learnable parameter count for this autoencoder by executing the last cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T20:03:21.368415Z",
     "start_time": "2020-08-18T20:03:19.286603Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 155
    },
    "colab_type": "code",
    "outputId": "c121605c-7e30-4e16-f09d-efe907d7cd67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder \n",
      "\n",
      " Sequential(\n",
      "  (0): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (1): PReLU(num_parameters=1)\n",
      "  (2): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (3): PReLU(num_parameters=1)\n",
      "  (4): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (5): PReLU(num_parameters=1)\n",
      "  (6): Linear(in_features=4, out_features=2, bias=True)\n",
      "  (7): PReLU(num_parameters=1)\n",
      "  (8): Linear(in_features=2, out_features=4, bias=True)\n",
      "  (9): PReLU(num_parameters=1)\n",
      "  (10): Linear(in_features=4, out_features=8, bias=True)\n",
      "  (11): PReLU(num_parameters=1)\n",
      "  (12): Linear(in_features=8, out_features=16, bias=True)\n",
      "  (13): PReLU(num_parameters=1)\n",
      "  (14): Linear(in_features=16, out_features=32, bias=True)\n",
      "  (15): Sigmoid()\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoding_size = 2#[2,4,8,16]\n",
    "input_size = 32\n",
    "model_32 = nn.Sequential(\n",
    "    nn.Linear(input_size, int(input_size / 2)),         #32\n",
    "    # Add activation function\n",
    "    nn.PReLU(),\n",
    "    # Add another layer\n",
    "    nn.Linear(int(input_size / 2), int(input_size / 4)),#16\n",
    "    # Add activation function\n",
    "    nn.PReLU(),\n",
    "    # Add another layer\n",
    "    nn.Linear(int(input_size / 4), int(input_size / 8)),#8\n",
    "    # Add activation function\n",
    "    nn.PReLU(),\n",
    "    # Add another layer\n",
    "    nn.Linear(int(input_size / 8), encoding_size),      #4\n",
    "    # Add activation function\n",
    "    nn.PReLU(),\n",
    "    # Add another layer\n",
    "    nn.Linear(encoding_size, int(input_size / 8)),     #2\n",
    "    # Add activation function\n",
    "    nn.PReLU(),\n",
    "    # Add another layer\n",
    "    nn.Linear(int(input_size / 8), int(input_size / 4)),#8\n",
    "    # Add activation function\n",
    "    nn.PReLU(), \n",
    "    # Add another layer\n",
    "    nn.Linear(int(input_size / 4), int(input_size / 2)),#16\n",
    "    # Add activation function\n",
    "    nn.PReLU(),\n",
    "    # Add another layer\n",
    "    nn.Linear(int(input_size / 2), input_size),#32    \n",
    "    # Add another layer\n",
    "    nn.Sigmoid()\n",
    "    )\n",
    "\n",
    "model_32[:-2].apply(init_weights_kaiming_normal)\n",
    "\n",
    "print(f'Autoencoder \\n\\n {model_32}\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Helper function:** `print_parameter_count`\n",
    "\n",
    "Please uncomment the line below to inspect this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T20:03:25.868681Z",
     "start_time": "2020-08-18T20:03:25.452331Z"
    },
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t 528\t Linear(in_features=32, out_features=16, bias=True)\n",
      "1\t 1\t PReLU(num_parameters=1)\n",
      "2\t 136\t Linear(in_features=16, out_features=8, bias=True)\n",
      "3\t 1\t PReLU(num_parameters=1)\n",
      "4\t 36\t Linear(in_features=8, out_features=4, bias=True)\n",
      "5\t 1\t PReLU(num_parameters=1)\n",
      "6\t 10\t Linear(in_features=4, out_features=2, bias=True)\n",
      "7\t 1\t PReLU(num_parameters=1)\n",
      "8\t 12\t Linear(in_features=2, out_features=4, bias=True)\n",
      "9\t 1\t PReLU(num_parameters=1)\n",
      "10\t 40\t Linear(in_features=4, out_features=8, bias=True)\n",
      "11\t 1\t PReLU(num_parameters=1)\n",
      "12\t 144\t Linear(in_features=8, out_features=16, bias=True)\n",
      "13\t 1\t PReLU(num_parameters=1)\n",
      "14\t 544\t Linear(in_features=16, out_features=32, bias=True)\n",
      "15\t 0\t Sigmoid()\n",
      "\n",
      "Total:\t 1457\n"
     ]
    }
   ],
   "source": [
    "print_parameter_count(model_32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Train the autoencoder\n",
    "\n",
    "Train the network for `n_epochs=10` epochs with `batch_size=128`, and observe how the internal representation successfully captures additional digit classes.\n",
    "\n",
    "The encoder map shows well-separated clusters that correspond to the associated digits in the decoder grid. The decoder grid also shows that the network is robust to digit skewness, i.e., digits leaning to the left or the right are recognized in the same digit class.\n",
    "\n",
    "**Instructions:**\n",
    "* Please execute the cells below\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T20:03:49.912667Z",
     "start_time": "2020-08-18T20:03:39.866769Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 620
    },
    "colab_type": "code",
    "outputId": "e0bc6059-b270-4a8f-9244-9cdbff1a898c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch \t Loss train \t Loss test\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [640 x 129329], m2: [32 x 16] at /tmp/pip-req-build-o37zi_pa/aten/src/TH/generic/THTensorMath.cpp:136",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d69893621b7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m runSGD(model_32, por_canal[0], por_canal[0], n_epochs=n_epochs,\n\u001b[0;32m----> 5\u001b[0;31m        batch_size=batch_size)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-cada271cde82>\u001b[0m in \u001b[0;36mrunSGD\u001b[0;34m(net, input_train, input_test, criterion, n_epochs, batch_size, verbose)\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 587\u001b[0;31m       \u001b[0moutput_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    588\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1372\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1373\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [640 x 129329], m2: [32 x 16] at /tmp/pip-req-build-o37zi_pa/aten/src/TH/generic/THTensorMath.cpp:136"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "runSGD(model_32, por_canal[0], por_canal[0], n_epochs=n_epochs,\n",
    "       batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T19:29:28.480430Z",
     "start_time": "2020-08-18T19:29:28.475327Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 784])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 639
    },
    "colab_type": "code",
    "outputId": "51da90b9-f8db-427e-e4ff-08d7d6e5e3da"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  output_test = model(input_test)\n",
    "  latent_test = encoder(input_test)\n",
    "\n",
    "plot_row([input_test[test_selected_idx], output_test[test_selected_idx]],\n",
    "         image_shape=image_shape)\n",
    "\n",
    "plot_latent_generative(latent_test, y_test, decoder, image_shape=image_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "---\n",
    "# Section 3: Spherical latent space\n",
    "\n",
    "The previous architecture generates representations that typically spread in different directions from coordinate $(z_1, z_2)=(0,0)$. This effect is due to the initialization of weights distributed randomly around `0`.\n",
    "\n",
    "Adding a third unit to the bottleneck layer defines a coordinate $(z_1, z_2, z_3)$ in 3D space. The latent space from such a network will still spread out from $(z_1, z_2, z_3)=(0, 0, 0)$.\n",
    "\n",
    "Collapsing the latent space on the surface of a sphere removes the possibility of spreading indefinitely from the origin $(0, 0, 0)$ in any direction since this will eventually lead back to the origin. This constraint generates a representation that fills the surface of the sphere.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "![Unit sphere S2](https://github.com/mpbrigham/colaboratory-figures/raw/master/nma/autoencoders/unit_sphere.png)\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "Projecting to the surface of the sphere is implemented by dividing the coordinates $(z_1, z_2, z_3)$ by their $L_2$ norm.\n",
    "\n",
    "$(z_1, z_2, z_3)\\longmapsto (s_1, s_2, s_3)=(z_1, z_2, z_3)/\\|(z_1, z_2, z_3)\\|_2=(z_1, z_2, z_3)/ \\sqrt{z_1^2+z_2^2+z_3^2}$\n",
    "\n",
    "This mapping projects to the surface of the [$S_2$ sphere](https://en.wikipedia.org/wiki/N-sphere) with unit radius. (Why?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Section 3.1: Build and train autoencoder (3D)\n",
    "\n",
    "We start by adding one unit to the bottleneck layer and visualize the latent space in 3D.\n",
    "\n",
    "Please execute the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "colab_type": "code",
    "outputId": "a8ad8bd3-1869-4922-aeb1-cc4f1a268c04"
   },
   "outputs": [],
   "source": [
    "encoding_size = 3\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(input_size, int(input_size / 2)),\n",
    "    nn.PReLU(),\n",
    "    nn.Linear(int(input_size / 2), encoding_size * 32),\n",
    "    nn.PReLU(),\n",
    "    nn.Linear(encoding_size * 32, encoding_size),\n",
    "    nn.PReLU(),\n",
    "    nn.Linear(encoding_size, encoding_size * 32),\n",
    "    nn.PReLU(),\n",
    "    nn.Linear(encoding_size * 32, int(input_size / 2)),\n",
    "    nn.PReLU(),\n",
    "    nn.Linear(int(input_size / 2), input_size),\n",
    "    nn.Sigmoid()\n",
    "    )\n",
    "\n",
    "model[:-2].apply(init_weights_kaiming_normal)\n",
    "\n",
    "encoder = model[:6]\n",
    "decoder = model[6:]\n",
    "\n",
    "print(f'Autoencoder \\n\\n {model}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Section 3.2: Train the autoencoder\n",
    "\n",
    "Train the network for `n_epochs=10` epochs with `batch_size=128`. Observe how the internal representation spreads from the origin and reaches much lower loss due to the additional degree of freedom in the bottleneck layer.\n",
    "\n",
    "**Instructions:**\n",
    "* Please execute the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 620
    },
    "colab_type": "code",
    "outputId": "4b2a8b10-7d74-4456-a01d-af7c19f9888f"
   },
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "runSGD(model, input_train, input_test, n_epochs=n_epochs,\n",
    "       batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Section 3.3: Visualize the latent space in 3D\n",
    "\n",
    "**Helper function**: `plot_latent_3d`\n",
    "\n",
    "Please uncomment the line below to inspect this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# help(plot_latent_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "colab_type": "code",
    "outputId": "fca4eff2-c292-4e0c-f79c-c7b83eab154e"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  latent_test = encoder(input_test)\n",
    "\n",
    "plot_latent_3d(latent_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Exercise 2: Build deep autoencoder (2D) with latent spherical space\n",
    "We now constrain the latent space to the surface of a sphere $S_2$.\n",
    "\n",
    "\n",
    "**Instructions:**\n",
    "* Add the custom layer `NormalizeLayer` after the bottleneck layer\n",
    "* Adjust the definitions of `encoder` and `decoder`\n",
    "* Experiment with keyword `show_text=False` for `plot_latent_3d`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Helper function**: `NormalizeLayer`\n",
    "\n",
    "Please uncomment the line below to inspect this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# help(NormalizeLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "colab_type": "code",
    "outputId": "97937cb2-e3d1-4e48-dc6e-612aabfda04c"
   },
   "outputs": [],
   "source": [
    "encoding_size = 3\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(input_size, int(input_size / 2)),\n",
    "    nn.PReLU(),\n",
    "    nn.Linear(int(input_size / 2), encoding_size * 32),\n",
    "    nn.PReLU(),\n",
    "    nn.Linear(encoding_size * 32, encoding_size),\n",
    "    nn.PReLU(),\n",
    "    #################################################\n",
    "    ## TODO for students: add custom normalize layer\n",
    "    #################################################\n",
    "    # add the normalization layer\n",
    "    # ...,\n",
    "    nn.Linear(encoding_size, encoding_size * 32),\n",
    "    nn.PReLU(),\n",
    "    nn.Linear(encoding_size * 32, int(input_size / 2)),\n",
    "    nn.PReLU(),\n",
    "    nn.Linear(int(input_size / 2), input_size),\n",
    "    nn.Sigmoid()\n",
    "    )\n",
    "\n",
    "model[:-2].apply(init_weights_kaiming_normal)\n",
    "\n",
    "print(f'Autoencoder \\n\\n {model}\\n')\n",
    "\n",
    "# Adjust the value n_l to split your model correctly\n",
    "# n_l = ...\n",
    "\n",
    "# uncomment when you fill the code\n",
    "# encoder = model[:n_l]\n",
    "# decoder = model[n_l:]\n",
    "# print(f'Encoder \\n\\n {encoder}\\n')\n",
    "# print(f'Decoder \\n\\n {decoder}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 708
    },
    "colab_type": "code",
    "outputId": "53820287-7f7b-4856-9132-8eaee803c863"
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "encoding_size = 3\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(input_size, int(input_size / 2)),\n",
    "    nn.PReLU(),\n",
    "    nn.Linear(int(input_size / 2), encoding_size * 32),\n",
    "    nn.PReLU(),\n",
    "    nn.Linear(encoding_size * 32, encoding_size),\n",
    "    nn.PReLU(),\n",
    "    # add the normalization layer\n",
    "    NormalizeLayer(),\n",
    "    nn.Linear(encoding_size, encoding_size * 32),\n",
    "    nn.PReLU(),\n",
    "    nn.Linear(encoding_size * 32, int(input_size / 2)),\n",
    "    nn.PReLU(),\n",
    "    nn.Linear(int(input_size / 2), input_size),\n",
    "    nn.Sigmoid()\n",
    "    )\n",
    "\n",
    "model[:-2].apply(init_weights_kaiming_normal)\n",
    "\n",
    "print(f'Autoencoder \\n\\n {model}\\n')\n",
    "\n",
    "# Adjust the value n_l to split your model correctly\n",
    "n_l = 7\n",
    "\n",
    "# uncomment when you fill the code\n",
    "encoder = model[:n_l]\n",
    "decoder = model[n_l:]\n",
    "print(f'Encoder \\n\\n {encoder}\\n')\n",
    "print(f'Decoder \\n\\n {decoder}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Section 3.4: Train the autoencoder\n",
    "Train the network for `n_epochs=10` epochs with `batch_size=128` and observe how loss raises again and is comparable to the model with 2D latent space.\n",
    "\n",
    "**Instructions:**\n",
    "* Please execute the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 620
    },
    "colab_type": "code",
    "outputId": "ecaaada8-ded4-4aac-e8eb-030e9038c09b"
   },
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "runSGD(model, input_train, input_test, n_epochs=n_epochs,\n",
    "       batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "colab_type": "code",
    "outputId": "632bd2bd-5398-403a-b62a-2149a99c7e30"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  latent_test = encoder(input_test)\n",
    "\n",
    "plot_latent_3d(latent_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Section 3.5: Visualize latent space on surface of $S_2$\n",
    "The 3D coordinates $(s_1, s_2, s_3)$ on the surface of the unit sphere $S_2$  can be mapped to [spherical coordinates](https://en.wikipedia.org/wiki/Spherical_coordinate_system) $(r, \\theta, \\phi)$, as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "r &= \\sqrt{s_1^2 + s_2^2 + s_3^2} \\\\\n",
    "\\phi &= \\arctan \\frac{s_2}{s_1} \\\\\n",
    "\\theta &= \\arccos\\frac{s_3}{r}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "![Spherical coordinates](https://github.com/mpbrigham/colaboratory-figures/raw/master/nma/autoencoders/spherical_coords.png)\n",
    "\n",
    "What is the domain (numerical range) spanned by ($\\theta, \\phi)$?\n",
    "\n",
    "We return to a 2D representation since the angles $(\\theta, \\phi)$ are the only degrees of freedom on the surface of the sphere. Add the keyword `s2=True` to `plot_latent_generative` to un-wrap the sphere's surface similar to a world map.\n",
    "\n",
    "Task: Check the numerical range of the plot axis to help identify $\\theta$ and $\\phi$, and visualize the unfolding of the 3D plot from the previous exercise.\n",
    "\n",
    "**Instructions:**\n",
    "* Please execute the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 639
    },
    "colab_type": "code",
    "outputId": "44bcc242-bd08-405e-85f6-dce654f995fe"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  output_test = model(input_test)\n",
    "\n",
    "plot_row([input_test[test_selected_idx], output_test[test_selected_idx]],\n",
    "         image_shape=image_shape)\n",
    "\n",
    "plot_latent_generative(latent_test, y_test, decoder,\n",
    "                       image_shape=image_shape, s2=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "---\n",
    "# Summary\n",
    "We learned two techniques to improve representation capacity: adding a few hidden layers and projecting latent space on the sphere $S_2$.\n",
    "\n",
    "The expressive power of autoencoder improves with additional hidden layers. Projecting latent space on the surface of $S_2$ spreads out digits classes in a more visually pleasing way but may not always produce a lower loss.\n",
    "\n",
    "**Deep autoencoder architectures have rich internal representations to deal with sophisticated tasks such as the MNIST cognitive task.**\n",
    "\n",
    "We now have powerful tools to explore how simple algorithms build robust models of the world by capturing relevant data patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "colab_type": "code",
    "outputId": "7265b876-62a2-4393-fed8-d1410badd997"
   },
   "outputs": [],
   "source": [
    "# @title Video 2: Wrap-up\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"GnkmzCqEK3E\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "---\n",
    "# Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Deep and thick autoencoder\n",
    "In this exercise, we first expand the first hidden layer to double the input size, followed by compression to half the input size leading to 3.8M parameters. Please **do not train this network during tutorial** due to long training time.\n",
    "\n",
    "**Instructions:**\n",
    "* Please uncomment and execute the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# encoding_size = 3\n",
    "\n",
    "# model = nn.Sequential(\n",
    "#     nn.Linear(input_size, int(input_size * 2)),\n",
    "#     nn.PReLU(),\n",
    "#     nn.Linear(int(input_size * 2), int(input_size / 2)),\n",
    "#     nn.PReLU(),\n",
    "#     nn.Linear(int(input_size / 2), encoding_size * 32),\n",
    "#     nn.PReLU(),\n",
    "#     nn.Linear(encoding_size * 32, encoding_size),\n",
    "#     nn.PReLU(),\n",
    "#     NormalizeLayer(),\n",
    "#     nn.Linear(encoding_size, encoding_size * 32),\n",
    "#     nn.PReLU(),\n",
    "#     nn.Linear(encoding_size * 32, int(input_size / 2)),\n",
    "#     nn.PReLU(),\n",
    "#     nn.Linear(int(input_size / 2), int(input_size * 2)),\n",
    "#     nn.PReLU(),\n",
    "#     nn.Linear(int(input_size * 2), input_size),\n",
    "#     nn.Sigmoid()\n",
    "#     )\n",
    "\n",
    "# model[:-2].apply(init_weights_kaiming_normal)\n",
    "\n",
    "# encoder = model[:9]\n",
    "# decoder = model[9:]\n",
    "\n",
    "# print_parameter_count(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# n_epochs = 5\n",
    "# batch_size = 128\n",
    "\n",
    "# runSGD(model, input_train, input_test, n_epochs=n_epochs,\n",
    "#        batch_size=batch_size)\n",
    "\n",
    "# Visualization\n",
    "# with torch.no_grad():\n",
    "#   output_test = model(input_test)\n",
    "\n",
    "# plot_row([input_test[test_selected_idx], output_test[test_selected_idx]],\n",
    "#          image_shape=image_shape)\n",
    "\n",
    "# plot_latent_generative(latent_test, y_test, decoder,\n",
    "#                        image_shape=image_shape, s2=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "JgF7_zvb8d0C"
   ],
   "include_colab_link": true,
   "name": "W3D5_Tutorial2",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

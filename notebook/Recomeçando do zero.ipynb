{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T19:06:48.921111Z",
     "start_time": "2020-09-01T19:06:48.436788Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brain/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, LeaveOneOut\n",
    "from tqdm.autonotebook import tqdm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report, cohen_kappa_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import scipy.io as sio\n",
    "import scipy as sc\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# %matplotlib outline\n",
    "sns.set()\n",
    "\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "sys.path.append(\"../src/data/\")\n",
    "sys.path.append(\"../src/tools/\")\n",
    "sys.path.append(\"../src/models/\")\n",
    "sys.path.append(\"../src/visualization/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T19:06:51.070739Z",
     "start_time": "2020-09-01T19:06:50.355039Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 items had no tests:\n",
      "    __main__\n",
      "0 tests in 1 items.\n",
      "0 passed and 0 failed.\n",
      "Test passed.\n"
     ]
    }
   ],
   "source": [
    "from file import read_file, get_bad_trials, get_bad_trials_comportamental, get_time_delay\n",
    "from conversion import to_DataFrame, split_exposure, merge_and_clean, merge_export, to_DataFrame_autoenconder\n",
    "from exposure import _exposure_1, _exposure_2, _exposure_1_bad, _exposure_2_bad, get_group_time, fixing_bad_trials, get_last_125ms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T19:07:09.886616Z",
     "start_time": "2020-09-01T19:06:57.253795Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brain/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/brain/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/brain/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/brain/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/brain/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/brain/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T19:07:54.428834Z",
     "start_time": "2020-09-01T19:07:53.834510Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device:/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T19:08:00.429654Z",
     "start_time": "2020-09-01T19:08:00.424478Z"
    }
   },
   "outputs": [],
   "source": [
    "N_PEOPLE = 20\n",
    "PATH_AUD = '../data/raw/aud'\n",
    "PATH_VIS = '../data/raw/vis'\n",
    "PATH_INFO = '../data/raw/info_'\n",
    "\n",
    "N_CHANNELS = 64\n",
    "#N_CHANNELS = 62\n",
    "\n",
    "ORIGINAL_FREQUENCY = 1000\n",
    "\n",
    "DOWN_SAMPLED_FREQUENCY = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T19:08:19.503240Z",
     "start_time": "2020-09-01T19:08:01.817062Z"
    }
   },
   "outputs": [],
   "source": [
    "data_aud, data_vis, CHANNEL_NAMES = read_file(PATH_AUD, PATH_VIS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T19:08:19.506242Z",
     "start_time": "2020-09-01T19:08:19.504351Z"
    }
   },
   "outputs": [],
   "source": [
    "## Splitting the exposures for all the individuals\n",
    "\n",
    "aud_1 = _exposure_1(data_aud)\n",
    "\n",
    "aud_2 = _exposure_2(data_aud) \n",
    "\n",
    "vis_1 = _exposure_1(data_vis)\n",
    "\n",
    "vis_2 = _exposure_2(data_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T19:08:31.061192Z",
     "start_time": "2020-09-01T19:08:19.507270Z"
    }
   },
   "outputs": [],
   "source": [
    "bad_trials_aud, bad_trials_vis = get_bad_trials(PATH_AUD, PATH_VIS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T19:08:31.071440Z",
     "start_time": "2020-09-01T19:08:31.062364Z"
    }
   },
   "outputs": [],
   "source": [
    "## Splitting the exposures for all the individuals\n",
    "\n",
    "bad_aud_1 = _exposure_1_bad(bad_trials_aud, 'Aud')\n",
    "\n",
    "bad_aud_2 = _exposure_2_bad(bad_trials_aud, 'Aud') \n",
    "\n",
    "bad_vis_1 = _exposure_1_bad(bad_trials_vis, 'Vis')\n",
    "\n",
    "bad_vis_2 = _exposure_2_bad(bad_trials_vis, 'Vis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T19:08:31.241435Z",
     "start_time": "2020-09-01T19:08:31.072242Z"
    }
   },
   "outputs": [],
   "source": [
    "bad_vis_comport = get_bad_trials_comportamental('vis')\n",
    "bad_aud_comport = get_bad_trials_comportamental('aud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T19:08:31.259723Z",
     "start_time": "2020-09-01T19:08:31.242363Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_aud_1, clean_aud_2, clean_vis_1, clean_vis_2 = fixing_bad_trials(\n",
    "    bad_aud_1, bad_aud_2, bad_vis_1, bad_vis_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T19:08:31.326707Z",
     "start_time": "2020-09-01T19:08:31.260470Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_aud_1 = clean_aud_1.append(bad_aud_comport,ignore_index=True)\n",
    "clean_aud_2 = clean_aud_2.append(bad_aud_comport,ignore_index=True)\n",
    "\n",
    "clean_vis_1 = clean_vis_1.append(bad_vis_comport,ignore_index=True)\n",
    "clean_vis_2 = clean_vis_2.append(bad_vis_comport,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T19:08:31.429520Z",
     "start_time": "2020-09-01T19:08:31.331946Z"
    }
   },
   "outputs": [],
   "source": [
    "indice_s2_vis = get_time_delay('vis')\n",
    "indice_s2_aud = get_time_delay('aud')\n",
    "\n",
    "time_s2_vis = get_time_delay('vis', export_as_indice=False)\n",
    "time_s2_aud = get_time_delay('aud', export_as_indice=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T19:08:31.642953Z",
     "start_time": "2020-09-01T19:08:31.432496Z"
    }
   },
   "outputs": [],
   "source": [
    "aud_1_average = list(map(get_last_125ms, aud_1, indice_s2_aud))\n",
    "aud_2_average = list(map(get_last_125ms, aud_2, indice_s2_aud))\n",
    "\n",
    "\n",
    "vis_1_average = list(map(get_last_125ms, vis_1, indice_s2_vis))\n",
    "vis_2_average = list(map(get_last_125ms, vis_2, indice_s2_vis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T19:08:31.739932Z",
     "start_time": "2020-09-01T19:08:31.644732Z"
    }
   },
   "outputs": [],
   "source": [
    "classes_aud, classes_vis = get_group_time(time_s2_aud, time_s2_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T19:08:31.901560Z",
     "start_time": "2020-09-01T19:08:31.740704Z"
    }
   },
   "outputs": [],
   "source": [
    "#Getting in format of dataframe:\n",
    "\n",
    "df_aud_1_aver = to_DataFrame(aud_1_average,classes_aud,CHANNEL_NAMES)\n",
    "df_aud_2_aver = to_DataFrame(aud_2_average,classes_aud,CHANNEL_NAMES)\n",
    "\n",
    "df_vis_1_aver = to_DataFrame(vis_1_average,classes_vis,CHANNEL_NAMES)\n",
    "df_vis_2_aver = to_DataFrame(vis_2_average,classes_vis,CHANNEL_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T18:43:24.555365Z",
     "start_time": "2020-09-01T18:43:24.549646Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T18:45:27.444614Z",
     "start_time": "2020-09-01T18:45:27.433079Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (101) : invalid device ordinal at /pytorch/torch/csrc/cuda/Module.cpp:59",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-a2e9ec55bc21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"cuda:{gpu}\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mset_device\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_device_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_setDevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (101) : invalid device ordinal at /pytorch/torch/csrc/cuda/Module.cpp:59"
     ]
    }
   ],
   "source": [
    "gpu=1\n",
    "device = torch.device(f\"cuda:{gpu}\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T17:21:01.144319Z",
     "start_time": "2020-09-01T17:21:01.138475Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T18:50:40.702302Z",
     "start_time": "2020-09-01T18:50:40.683132Z"
    }
   },
   "outputs": [],
   "source": [
    "def eval_mse(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Evaluates mean square error (MSE) between y_pred and y_true\n",
    "\n",
    "    Args:\n",
    "      y_pred (torch.Tensor)\n",
    "          prediction samples\n",
    "\n",
    "      v (numpy array of floats)\n",
    "          ground truth samples\n",
    "\n",
    "    Returns:\n",
    "      MSE(y_pred, y_true)\n",
    "    \"\"\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        criterion = nn.MSELoss()\n",
    "        loss = criterion(y_pred, y_true)\n",
    "\n",
    "    return float(loss)\n",
    "\n",
    "\n",
    "def eval_bce(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Evaluates binary cross-entropy (BCE) between y_pred and y_true\n",
    "\n",
    "    Args:\n",
    "      y_pred (torch.Tensor)\n",
    "          prediction samples\n",
    "\n",
    "      v (numpy array of floats)\n",
    "          ground truth samples\n",
    "\n",
    "    Returns:\n",
    "      BCE(y_pred, y_true)\n",
    "    \"\"\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        criterion = nn.BCELoss()\n",
    "        loss = criterion(y_pred, y_true)\n",
    "\n",
    "    return float(loss)\n",
    "\n",
    "\n",
    "def runSGD(net, input_train, input_test, criterion='bce',\n",
    "           n_epochs=10, batch_size=32, verbose=False):\n",
    "    \"\"\"\n",
    "    Trains autoencoder network with stochastic gradient descent with Adam\n",
    "    optimizer and loss criterion. Train samples are shuffled, and loss is\n",
    "    displayed at the end of each opoch for both MSE and BCE. Plots training loss\n",
    "    at each minibatch (maximum of 500 randomly selected values).\n",
    "\n",
    "    Args:\n",
    "      net (torch network)\n",
    "          ANN object (nn.Module)\n",
    "\n",
    "      input_train (torch.Tensor)\n",
    "          vectorized input images from train set\n",
    "\n",
    "      input_test (torch.Tensor)\n",
    "          vectorized input images from test set\n",
    "\n",
    "      criterion (string)\n",
    "          train loss: 'bce' or 'mse'\n",
    "\n",
    "      n_epochs (boolean)\n",
    "          number of full iterations of training data\n",
    "\n",
    "      batch_size (integer)\n",
    "          number of element in mini-batches\n",
    "\n",
    "      verbose (boolean)\n",
    "          print final loss\n",
    "\n",
    "    Returns:\n",
    "      Nothing.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize loss function\n",
    "    if criterion == 'mse':\n",
    "        loss_fn = nn.MSELoss()\n",
    "    elif criterionpytotch == 'bce':\n",
    "        loss_fn = nn.BCELoss()\n",
    "    else:\n",
    "        print('Please specify either \"mse\" or \"bce\" for loss criterion')\n",
    "\n",
    "    # Initialize SGD optimizer\n",
    "    optimizer = optim.Adam(net.parameters())\n",
    "\n",
    "    # Placeholder for loss\n",
    "    track_loss = []\n",
    "\n",
    "    # print('Epoch', '\\t', 'Loss train', '\\t', 'Loss test')\n",
    "    for i in range(n_epochs):\n",
    "\n",
    "        shuffle_idx = np.random.permutation(len(input_train))\n",
    "        batches = torch.split(input_train[shuffle_idx], batch_size)\n",
    "\n",
    "        for batch in batches:\n",
    "\n",
    "            output_train = net(batch).to(device)\n",
    "            loss = loss_fn(output_train, batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Keep track of loss at each epoch\n",
    "            track_loss += [float(loss)]\n",
    "\n",
    "        loss_epoch = f'{i+1}/{n_epochs}'\n",
    "        with torch.no_grad():\n",
    "            output_train = net(input_train).cuda()\n",
    "            loss_train = loss_fn(output_train, input_train)\n",
    "            loss_epoch += f'\\t {loss_train:.4f}'\n",
    "            output_test = net(input_test.cuda()).to(device)\n",
    "            loss_test = loss_fn(output_test, input_test)\n",
    "            loss_epoch += f'\\t\\t {loss_test:.4f}'\n",
    "\n",
    "        # print(loss_epoch)    \n",
    "    \n",
    "    if verbose:\n",
    "        # Print loss\n",
    "        loss_mse = f'\\nMSE\\t {eval_mse(output_train, input_train):0.4f}'\n",
    "        loss_mse += f'\\t\\t {eval_mse(output_test, input_test):0.4f}'\n",
    "        print(loss_mse)\n",
    "\n",
    "        loss_bce = f'BCE\\t {eval_bce(output_train, input_train):0.4f}'\n",
    "        loss_bce += f'\\t\\t {eval_bce(output_test, input_test):0.4f}'\n",
    "        print(loss_bce)\n",
    "\n",
    "        # Plot loss\n",
    "        step = int(np.ceil(len(track_loss) / 500))\n",
    "        x_range = np.arange(0, len(track_loss), step)\n",
    "        plt.figure()\n",
    "        plt.plot(x_range, track_loss[::step], 'C0')\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlim([0, None])\n",
    "        plt.ylim([0, None])\n",
    "        plt.show()\n",
    "\n",
    "    return net\n",
    "\n",
    "class NormalizeLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    pyTorch layer (nn.Module) that normalizes activations by their L2 norm.\n",
    "\n",
    "    Args:\n",
    "        None.\n",
    "\n",
    "    Returns:\n",
    "        Object inherited from nn.Module class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.functional.normalize(x, p=2, dim=1)\n",
    "\n",
    "\n",
    "def init_weights_kaiming_normal(layer):\n",
    "    \"\"\"\n",
    "    Initializes weights from linear PyTorch layer\n",
    "    with kaiming normal distribution.\n",
    "\n",
    "    Args:\n",
    "      layer (torch.Module)\n",
    "          Pytorch layer\n",
    "\n",
    "    Returns:nossa\n",
    "      Nothing.\n",
    "    \"\"\"\n",
    "    # check for linear PyTorch layer\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        # initialize weights with kaiming normal distribution\n",
    "        nn.init.kaiming_normal_(layer.weight.data,dtype=)\n",
    "\n",
    "\n",
    "def init_weights_kaiming_uniform(layer):\n",
    "    \"\"\"\n",
    "    Initializes weights from linear PyTorch layer\n",
    "    with kaiming uniform distribution.\n",
    "\n",
    "    Args:\n",
    "      layer (torch.Module)\n",
    "          Pytorch layer\n",
    "\n",
    "    Returns:\n",
    "      Nothing.\n",
    "    \"\"\"\n",
    "    # check for linear PyTorch layer\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        # initialize weights with kaiming uniform distribution\n",
    "        nn.init.kaiming_uniform_(layer.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T18:54:49.710700Z",
     "start_time": "2020-09-01T18:54:49.704000Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T18:50:37.153725Z",
     "start_time": "2020-09-01T18:50:37.129962Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder \n",
      "\n",
      " Sequential(\n",
      "  (0): Linear(in_features=62, out_features=31, bias=True)\n",
      "  (1): PReLU(num_parameters=1)\n",
      "  (2): Linear(in_features=31, out_features=15, bias=True)\n",
      "  (3): PReLU(num_parameters=1)\n",
      "  (4): Linear(in_features=15, out_features=7, bias=True)\n",
      "  (5): PReLU(num_parameters=1)\n",
      "  (6): Linear(in_features=7, out_features=2, bias=True)\n",
      "  (7): PReLU(num_parameters=1)\n",
      "  (8): Linear(in_features=2, out_features=7, bias=True)\n",
      "  (9): PReLU(num_parameters=1)\n",
      "  (10): Linear(in_features=7, out_features=15, bias=True)\n",
      "  (11): PReLU(num_parameters=1)\n",
      "  (12): Linear(in_features=15, out_features=31, bias=True)\n",
      "  (13): PReLU(num_parameters=1)\n",
      "  (14): Linear(in_features=31, out_features=62, bias=True)\n",
      "  (15): Sigmoid()\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoding_size = 2#[2,4,8,16]\n",
    "input_size = 62\n",
    "model_32 = nn.Sequential(\n",
    "    nn.Linear(input_size, int(input_size / 2)),         #32\n",
    "    # Add activation function\n",
    "    nn.PReLU(),\n",
    "    # Add another layer\n",
    "    nn.Linear(int(input_size / 2), int(input_size / 4)),#16\n",
    "    # Add activation function\n",
    "    nn.PReLU(),\n",
    "    # Add another layer\n",
    "    nn.Linear(int(input_size / 4), int(input_size / 8)),#8\n",
    "    # Add activation function\n",
    "    nn.PReLU(),\n",
    "    # Add another layer\n",
    "    nn.Linear(int(input_size / 8), encoding_size),      #4\n",
    "    # Add activation function\n",
    "    nn.PReLU(),\n",
    "    # Add another layer\n",
    "    nn.Linear(encoding_size, int(input_size / 8)),     #2\n",
    "    # Add activation function\n",
    "    nn.PReLU(),\n",
    "    # Add another layer\n",
    "    nn.Linear(int(input_size / 8), int(input_size / 4)),#8\n",
    "    # Add activation function\n",
    "    nn.PReLU(), \n",
    "    # Add another layer\n",
    "    nn.Linear(int(input_size / 4), int(input_size / 2)),#16\n",
    "    # Add activation function\n",
    "    nn.PReLU(),\n",
    "    # Add another layeré´\n",
    "    nn.Linear(int(input_size / 2), input_size),#32    \n",
    "    # Add another layer\n",
    "    nn.Sigmoid()\n",
    "    )\n",
    "match_case=False\n",
    "model_32[:-2].apply(init_weights_kaiming_normal)\n",
    "\n",
    "print(f'Autoencoder \\n\\n {model_32}\\n')\n",
    "\n",
    "\n",
    "model_32 = model_32.to(device)\n",
    "\n",
    "torch.backends.cudnn.benchmark=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T19:11:24.736569Z",
     "start_time": "2020-09-01T19:11:24.641104Z"
    }
   },
   "outputs": [],
   "source": [
    "from my_autoenconder import AutoEnconder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T18:56:08.869099Z",
     "start_time": "2020-09-01T18:56:08.860009Z"
    }
   },
   "outputs": [],
   "source": [
    "def classification_within_modality_auto(dataFrame, categoria, exposure):\n",
    "    '''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    dataFrame_result = []\n",
    "    loo = LeaveOneOut()\n",
    "\n",
    "    pbar = tqdm(total=loo.get_n_splits(dataFrame))\n",
    "\n",
    "    for ind, pearson in dataFrame.groupby('people'):\n",
    "\n",
    "        X = pearson.drop(['trial', 'group', 'people'], 1)\n",
    "        y = pearson['group']\n",
    "\n",
    "        loo = LeaveOneOut()\n",
    "\n",
    "        for train_index, test_index in loo.split(X):\n",
    "\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "            # Normalize\n",
    "            train_mean = average(X_train, axis=0)\n",
    "\n",
    "            X_train_without_mean = subtract(X_train, train_mean)\n",
    "            X_test_without_mean = subtract(X_test, train_mean)\n",
    "\n",
    "            clf = GaussianNB()\n",
    "\n",
    "            clf.class_prior_ = [(1/6), (1/6), (1/6), (1/6), (1/6), (1/6)]\n",
    "\n",
    "            # print(X_train_without_mean.to_numpy().shape)\n",
    "\n",
    "            my_autoenconder\n",
    "            \n",
    "            ae = runSGD(model_32, X_train_torch,\n",
    "                        X_test_torch,\n",
    "                        criterion='mse', n_epochs=100, batch_size=16, verbose=False)\n",
    "\n",
    "            encoder = ae[:7]\n",
    "            \n",
    "            with torch.no_grad():\n",
    "\n",
    "                X_train_pca = encoder(X_train_torch)\n",
    "                X_test_pca = encoder(X_test_torch)\n",
    "\n",
    "            clf = clf.fit(X_train_pca, y_train)\n",
    "\n",
    "            y_pred = clf.predict(X_test_pca)\n",
    "\n",
    "            dataFrame_result.append(\n",
    "                [ind, y_pred, y_test.values, categoria, exposure])\n",
    "            \n",
    "            #print(dataFrame_result)\n",
    "            pbar.update(1)\n",
    "\n",
    "    return dataFrame_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T18:43:08.066216Z",
     "start_time": "2020-09-01T18:43:08.060565Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "from tqdm.autonotebook import tqdm\n",
    "from numpy import average, subtract\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.decomposition import PCA\n",
    "from pandas import DataFrame, concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T18:43:09.591490Z",
     "start_time": "2020-09-01T18:43:09.572765Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T18:56:30.637915Z",
     "start_time": "2020-09-01T18:56:30.607583Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bc2188a7300477baaa2893336d74926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2400.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: an illegal memory access was encountered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-3d2af5fe92f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclassification_within_modality_auto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_aud_1_aver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Auditory'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'E1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-46-c40c15d3785a>\u001b[0m in \u001b[0;36mclassification_within_modality_auto\u001b[0;34m(dataFrame, categoria, exposure)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             X_train_torch = torch.from_numpy(\n\u001b[0;32m---> 40\u001b[0;31m                 X_train_without_mean.to_numpy().astype(np.float32)).to(device)\n\u001b[0m\u001b[1;32m     41\u001b[0m             X_test_torch = torch.from_numpy(\n\u001b[1;32m     42\u001b[0m                 X_test_without_mean.to_numpy().astype(np.float32)).to(device)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: an illegal memory access was encountered"
     ]
    }
   ],
   "source": [
    "classification_within_modality_auto(df_aud_1_aver, 'Auditory','E1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T18:50:00.873857Z",
     "start_time": "2020-09-01T18:50:00.868815Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T18:52:34.183581Z",
     "start_time": "2020-09-01T18:52:34.173869Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-a9fba1999899>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tensor' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T16:07:18.541654Z",
     "start_time": "2020-09-01T16:07:18.533125Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytor_gpu",
   "language": "python",
   "name": "pytorch_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
